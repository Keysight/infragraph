{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Modern AI systems, comprising diverse scale-up and scale-out interconnect topologies that integrate complex heterogeneous components, connected together via diverse means, face a lack of standardized overall infrastructure description, all which hinders benchmarking, simulation, and emulation.</p> <p>This documentation covers the following:</p>"},{"location":"#background","title":"Background","text":"<p>The background details and justification behind the schema.</p>"},{"location":"#chakra-ecosystem","title":"Chakra Ecosystem","text":"<p>The Chakra Ecosystem talks about the Chakra + InfraGraph ecosystem. How they complements each other to describe AI workloads and underlying infrastructures respectively.</p>"},{"location":"#schema","title":"Schema","text":"<p>An overview of the schema and it's repository.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>A simple generic case study demonstrating how to get started.</p>"},{"location":"#blueprints","title":"Blueprints","text":"<p>This section defines blueprints for devices and fabrics, enabling users to model various vendor-specific devices and standard fabric architectures.</p>"},{"location":"#extending-the-case-study-with-additional-data","title":"Extending the case study with additional data","text":"<p>This section provides a comprehensive guide on how a user can annotate various parts of infrastructre and add more details like DeviceType, Rank Identifier and so on.</p> <p>It covers the model description with examples for binding physical attributes with the logical infrastructure definition.</p>"},{"location":"#services","title":"Services","text":"<p>The site provides services for <code>validating</code> concrete instances of schemas.</p>"},{"location":"#advanced-examples","title":"Advanced Examples","text":"<p>Advanced examples include such things as:</p> <ul> <li>complex servers</li> <li>composable devices</li> <li>scaleup/scaleout infrastructure.</li> </ul>"},{"location":"#specification-browser","title":"Specification Browser","text":"<p>The API and Models can easily be reviewed using the auto-generated OpenAPI model and redocly html documentation.</p>"},{"location":"#community","title":"Community","text":"<p>Use our community resources to get help with InfraGraph on Github</p>"},{"location":"annotate/","title":"The <code>annotate_graph</code> API","text":""},{"location":"annotate/#overview","title":"Overview","text":"<p>Once the infrastructure or system of systems has been defined by using the <code>set_graph</code> API the base graph can be extended by using the <code>annotate_graph</code> API to add additional data using nodes and edges as endpoints for the data.</p> <p>The main objective of the <code>annotate_graph</code> API is to separate the infrastructure model from specific use-case models by allowing the graph to be extended with any type of data.  This ensures that <code>InfraGraph</code> does not morph into an attempt to define every nuance present in a system of systems.</p> <p>Any annotation efforts can always be proposed as model or service enhancements by submitting issues or pull requests to the InfraGraph repository.</p>"},{"location":"annotate/#additional-data","title":"Additional Data","text":"<p>Some examples of additional data are:   - AI data such as:     - ranks     - communication groups   - Configuration data such as:     - network interface card settings     - device addresses     - device routing tables</p> <p>The following code examples demonstrates how to use the <code>query_graph</code> API in conjunction with the <code>annotate_graph</code> API to extend the graph with additional user specific data.</p>"},{"location":"annotate/#adding-rank-data","title":"Adding <code>rank</code> data","text":"<p>In the Getting Started example, the instances of the <code>Server</code> device were created with the name of <code>host</code> and each instance having a specific number of components with a name of <code>xpu</code>.</p> <p>The following code demonstrates adding a <code>rank</code> attribute to every <code>host</code> instance that has a component with the name of <code>xpu</code>.</p> Add a rank to each host xpu <pre><code>import pytest\nfrom infragraph import *\nfrom infragraph.blueprints.fabrics.closfabric import ClosFabric\nfrom infragraph.infragraph_service import InfraGraphService\n\n\n@pytest.mark.asyncio\nasync def test_rank_annotations():\n    \"\"\"Test adding a rank attribute to every xpu node\"\"\"\n    # create the graph\n    service = InfraGraphService()\n    service.set_graph(ClosFabric())\n\n    # query the graph for host npus\n    npu_request = QueryRequest()\n    filter = npu_request.node_filters.add(name=\"xpu filter\")\n    filter.choice = QueryNodeFilter.ID_FILTER\n    filter.id_filter.operator = QueryNodeId.REGEX\n    filter.id_filter.value = r\"host\\.\\d+\\.xpu\\.\\d+\"\n    npu_response = service.query_graph(npu_request)\n\n    # annotate the graph\n    annotate_request = AnnotateRequest()\n    for idx, match in enumerate(npu_response.node_matches):\n        annotate_request.nodes.add(name=match.id, attribute=\"rank\", value=str(idx))\n    service.annotate_graph(annotate_request)\n\n    # query the graph for rank attributes\n    rank_request = QueryRequest()\n    filter = rank_request.node_filters.add(name=\"rank filter\")\n    filter.choice = QueryNodeFilter.ATTRIBUTE_FILTER\n    filter.attribute_filter.name = \"rank\"\n    filter.attribute_filter.operator = QueryNodeId.REGEX\n    filter.attribute_filter.value = r\"\\d+\"\n    rank_response = service.query_graph(rank_request)\n\n    # validation\n    assert len(npu_response.node_matches) &gt; 0\n    assert len(npu_response.node_matches) == len(annotate_request.nodes)\n    assert len(annotate_request.nodes) == len(rank_response.node_matches)\n\n\nif __name__ == \"__main__\":\n    pytest.main([\"-s\", __file__])\n</code></pre>"},{"location":"annotate/#adding-ipaddress-data","title":"Adding <code>ipaddress</code> data","text":"<p>In the Getting Started example, the instances of the <code>Server</code> device were created with the name of <code>host</code> and each instance having a <code>mgmt</code> nic component.</p> <p>The following code demonstrates adding an <code>ipaddress</code> attribute to the <code>host</code> instance <code>mgmt</code> nic.</p> Add an ipaddress to each host mgmt component <pre><code>import pytest\nimport ipaddress\nfrom infragraph import *\nfrom infragraph.blueprints.fabrics.closfabric import ClosFabric\nfrom infragraph.infragraph_service import InfraGraphService\n\n\n@pytest.mark.asyncio\nasync def test_ipaddress_annotations():\n    \"\"\"Test adding an ipaddress attribute to every server nic node\"\"\"\n    # create the graph\n    service = InfraGraphService()\n    service.set_graph(ClosFabric())\n\n    # query the graph for host nics\n    npu_request = QueryRequest()\n    filter = npu_request.node_filters.add(name=\"mgmt nic filter\")\n    filter.choice = QueryNodeFilter.ATTRIBUTE_FILTER\n    filter.attribute_filter.name = \"type\"\n    filter.attribute_filter.operator = QueryNodeId.EQ\n    filter.attribute_filter.value = \"mgmt-nic\"\n    nic_response = service.query_graph(npu_request)\n    print(nic_response.node_matches)\n\n    # annotate the graph\n    annotate_request = AnnotateRequest()\n    for idx, match in enumerate(nic_response.node_matches):\n        annotate_request.nodes.add(\n            name=match.id,\n            attribute=\"ipaddress\",\n            value=str(ipaddress.ip_address(idx)),\n        )\n    service.annotate_graph(annotate_request)\n\n    # query the graph for ipaddress attributes\n    ipaddress_request = QueryRequest()\n    filter = ipaddress_request.node_filters.add(name=\"ipaddress filter\")\n    filter.choice = QueryNodeFilter.ATTRIBUTE_FILTER\n    filter.attribute_filter.name = \"ipaddress\"\n    filter.attribute_filter.operator = QueryNodeId.REGEX\n    filter.attribute_filter.value = r\".*\"\n    ipaddress_response = service.query_graph(ipaddress_request)\n    print(ipaddress_response.node_matches)\n\n    # validation\n    assert len(nic_response.node_matches) &gt; 0\n    assert len(nic_response.node_matches) == len(annotate_request.nodes)\n    assert len(annotate_request.nodes) == len(ipaddress_response.node_matches)\n\n\nif __name__ == \"__main__\":\n    pytest.main([\"-s\", __file__])\n</code></pre>"},{"location":"background/","title":"Background","text":""},{"location":"background/#the-challenge-in-system-description","title":"The Challenge in System Description","text":"<p>Building and managing AI/HPC systems today feels a bit like untangling a giant ball of yarn.</p> <p>Modern AI systems integrate <code>complex, heterogeneous components like compute, memory, and storage</code>, all connected by diverse scale-up and scale-out interconnect topologies. Think of the <code>intricate networks</code> in a large AI factory \u2013 it's a mix of different technologies and topologies.\"</p> <p>But, a clear, <code>standardized</code> way to describe this overall infrastructure is missing. This isn't just a technical challenge; it creates real problems for everyone involved. It makes it incredibly difficult to benchmark, to simulate 'what-if' scenarios, or even to manage these systems efficiently. The result? <code>Significant operational overhead</code>, often <code>low hardware utilization</code>, and a <code>lot of frustration</code> for those trying to get these powerful systems to perform.\"</p> <p>We believe <code>capturing this diversity, this inherent complexity, in a structured way</code> is the first step to reasoning with it.\"</p>"},{"location":"background/#why-standardize","title":"Why Standardize?","text":"<p>Why go through all this effort? Because we want to move beyond just 'making it work' to truly 'optimizing it.' Our goal is to transform massive AI clusters from monolithic problems into structured, analyzable systems. When you have a <code>standardized description</code>, you can feed that information directly into your tools \u2013 your simulators, your emulators, even your deployment systems.</p> <p>This means better predictions about how your AI workloads will perform before you even run them. And it opens the door for something really powerful when doing <code>vertical co-design.</code> It's about making sure your hardware and software are talking to each other, working together seamlessly, from the lowest-level chip to the highest-level application. This ensures accurate performance prediction and validates AI workloads pre-deployment.</p> <p>This isn't just about efficiency; it's about building performant, reliable, and cost-effective AI infrastructure that can keep up with the demands of tomorrow and experimenting with what-if scenarios.  Let's look at a concrete example.</p>"},{"location":"background/#from-schema-to-simulationemulation","title":"From Schema to Simulation/Emulation","text":"<p>So, here's how we put it all together. The challenge is that these complex AI systems are incredibly difficult to model accurately. Say you have a particular infrastructure benchmark like looking at how collective libraries will perform given different design choices, A vs. B.</p> <p>Our solution involves combining two key pieces: our <code>InfraGraph schema</code>, which precisely defines the cluster's topology, and the MLCommons Chakra workload traces, which capture the actual behavior of AI applications.</p> <p>Together, these standardized inputs feed directly into powerful simulators like AstraSim. This isn't just theoretical; it's a working proof point. It allows us to run detailed 'what-if' analyses, explore different design choices of how a cluster is composed together, and truly understand performance trade-offs.</p> <p>This capability effectively <code>democratizes</code> the evaluation of complex AI systems, putting powerful analysis tools into more hands.</p> <ul> <li>infrastructure schema + MLCommons Chakra<ul> <li>standardized <code>infrastructure schema</code> defines system of systems</li> <li><code>chakra</code> provides workload traces</li> </ul> </li> <li>topology input for various tools (ASTRA-Sim)</li> <li>enabled <code>what-if</code> analysis, design choices</li> <li>democratizes complex system evaluation</li> <li>an <code>infrastructure schema</code> is a flexible starting point today</li> </ul>"},{"location":"blueprints/","title":"Infragraph Blueprints","text":"<p>Infragraph provides users with multiple blueprints that help define the foundational components of an infrastructure. These blueprints cover both devices and fabrics, enabling flexible and extensible modeling of network and compute infrastructure.</p> <p>Each blueprint is implemented as Python source code, allowing users to create classes and objects that inherit from the generated SDK. This approach makes it possible to define more realistic and customizable representations of devices and fabrics while maintaining compatibility with Infragraph\u2019s schema and data models.</p> <p>All available blueprints can be found in the following directory:</p> <pre><code>src/infragraph/blueprints\n</code></pre>"},{"location":"blueprints/#device-blueprints","title":"Device Blueprints","text":"<p>Device blueprints enable you to instantiate specific device types using the Infragraph schema format. Each blueprint encapsulates attributes, component definitions, and configuration logic that represent actual hardware or node types within a network topology.</p> <p>All device blueprints are organized under: <pre><code>src/infragraph/blueprints/devices/&lt;vendor&gt;\n</code></pre></p> <p>Each Infragraph device can expose multiple variants, allowing the same blueprint to represent different SKUs or generations of hardware. For example, a <code>QSFP</code> device supports the following variants: - <code>qsfp_plus_40g</code> - <code>qsfp28_100g</code> - <code>qsfp56_200g</code> - <code>qsfp_dd_400g</code> - <code>qsfp_dd_800g</code></p> <p>Infragraph devices support variant selection at initialization time, enabling end users to choose the exact hardware variant they want to model when creating a device object.</p> <p>The example below shows QSFP class definition and initialization using a specific variant, demonstrating how variants are applied in practice.</p>"},{"location":"blueprints/#creating-a-qsfp-transceiver-blueprint","title":"Creating a QSFP Transceiver Blueprint","text":"<p>The <code>QSFP</code> class defines a device blueprint for QSFP-family pluggable transceivers in InfraGraph. It inherits from the base <code>Device</code> class and models the internal structure, ports, and signal binding of a QSFP module in a consistent and reusable way.</p> <p>All QSFP variants share the same internal topology:</p> <ul> <li>One electrical host-facing interface</li> <li>One optical media-facing interface</li> <li>A fixed one-to-one electrical \u2194 optical binding</li> </ul> <p>Only the capabilities (form factor, speed, and lane count) differ across variants, and these are strictly controlled using a variant catalog.</p>"},{"location":"blueprints/#example-qsfppy","title":"Example: <code>qsfp.py</code>","text":"<p><code>Location: /src/infragraph/blueprints/devices/common/transceiver</code></p> QSFP device definition using OpenApiArt generated classes <pre><code>from typing import Literal, Dict\nfrom infragraph import *\n\nQsfpVariant = Literal[\n    \"qsfp_plus_40g\",\n    \"qsfp28_100g\",\n    \"qsfp56_200g\",\n    \"qsfp_dd_400g\",\n    \"qsfp_dd_800g\"\n]\n\nQSFP_VARIANT_CATALOG: Dict[QsfpVariant, dict] = {\n    \"qsfp_plus_40g\": {\n        \"form_factor\": \"QSFP_PLUS\",\n        \"speed\": \"40Gbps\",\n        \"lanes\": 4,\n    },\n    \"qsfp28_100g\": {\n        \"form_factor\": \"QSFP28\",\n        \"speed\": \"100Gbps\",\n        \"lanes\": 4,\n    },\n    \"qsfp56_200g\": {\n        \"form_factor\": \"QSFP56\",\n        \"speed\": \"200Gbps\",\n        \"lanes\": 4,\n    },\n    \"qsfp_dd_400g\": {\n        \"form_factor\": \"QSFP-DD\",\n        \"speed\": \"400Gbps\",\n        \"lanes\": 8,\n    },\n    \"qsfp_dd_800g\": {\n        \"form_factor\": \"QSFP-DD\",\n        \"speed\": \"800Gbps\",\n        \"lanes\": 8,\n    }\n}\n\nclass QSFP(Device):\n    \"\"\"\n    InfraGraph model of a QSFP-family pluggable transceiver.\n\n    This class represents QSFP+, QSFP28, QSFP56, QSFP-DD 400 &amp; 800G\n    optical modules. All variants share the same internal topology:\n    a one-to-one binding between an electrical host interface and\n    an optical media interface.\n\n    Variants are profile-locked via an authoritative catalog and\n    differ only in bandwidth, signaling lanes, and form factor.\n    \"\"\"\n\n    def __init__(self, variant: QsfpVariant = \"qsfp28_100g\"):\n        \"\"\"\n        Initialize a QSFP transceiver model.\n\n        Args:\n            variant (QsfpVariant, optional):\n                QSFP variant to instantiate. Determines form factor,\n                total bandwidth, and number of electrical lanes.\n                Defaults to \"qsfp28_100g\".\n\n        Raises:\n            ValueError:\n                If an unsupported QSFP variant is specified.\n        \"\"\"\n        super(Device, self).__init__()\n\n        self._validate_variant(variant)\n\n        self.variant = variant\n        self.cfg = QSFP_VARIANT_CATALOG[variant]\n\n        self.name = self.cfg[\"form_factor\"].lower().replace(\"-\", \"_\")\n        self.description = f\"{self.cfg['form_factor']} {self.cfg['speed']} Transceiver\"\n\n        self.electrical_port = self._add_electrical_port()\n        self.optical_port = self._add_optical_port()\n        self.binding = self._add_links()\n\n        self._wire_internal()\n\n    def _validate_variant(self, variant: QsfpVariant):\n        if variant not in QSFP_VARIANT_CATALOG:\n            raise ValueError(f\"Unsupported QSFP variant: {variant}\")\n\n    def _add_electrical_port(self):\n        port = self.components.add(\n            name=\"electrical_port\",\n            description=f\"Electrical host interface ({self.cfg['lanes']} lanes)\",\n            count=1,\n        )\n        port.choice = Component.PORT\n        return port\n\n    def _add_optical_port(self):\n        port = self.components.add(\n            name=\"optical_port\",\n            description=f\"Optical media interface ({self.cfg['speed']})\",\n            count=1,\n        )\n        port.choice = Component.PORT\n        return port\n\n    def _add_links(self):\n        return self.links.add(\n            name=\"internal_binding\",\n            description=\"Electrical-to-optical signal binding\",\n        )\n\n    def _wire_internal(self):\n        edge = self.edges.add(\n            scheme=DeviceEdge.ONE2ONE,\n            link=self.binding.name,\n        )\n        edge.ep1.component = \"electrical_port\"\n        edge.ep2.component = \"optical_port\"\n\nif __name__ == \"__main__\":\n    print(QSFP(\"qsfp_dd_400g\").serialize(encoding=Device.YAML))\n</code></pre>"},{"location":"blueprints/#composability","title":"Composability","text":"<p>Infragraph supports modeling a device inside another device, where the nested device behaves as a component of the parent device using <code>Component.Device</code>.</p> <p>The idea is that we can add a CX5 device to a DGX, and then add a QSFP to the CX5:</p>"},{"location":"blueprints/#example-composing-dgx-cx5-qsfp","title":"Example: Composing DGX-CX5-QSFP","text":"DGX composed of CX5 which has QSFP as a device <pre><code>qsfp = QSFP(\"qsfp28_100g\")\ncx5 = Cx5(variant=\"cx5_100g_single\", transceiver=qsfp)\ndgx = NvidiaDGX(\"dgx_h100\", cx5)\ninfrastructure = Api().infrastructure()\n# we need to append added devices\ninfrastructure.devices.append(dgx).append(cx5).append(qsfp)\ninfrastructure.instances.add(name=dgx.name, device=dgx.name, count=1)\nservice = InfraGraphService()\nservice.set_graph(infrastructure)\ng = service.get_networkx_graph()\n</code></pre> <p>This creates a single DGX h100 variant composed of a CX5 100g single port which is composed of a QSFP 28 100g. Generated YAML:</p> DGX-CX5-QSFP composed device definition as yaml <pre><code>devices:\n- components:\n  - choice: cpu\n    count: 2\n    description: AMD EPYC 9654 (Genoa)\n    name: cpu\n  - choice: xpu\n    count: 8\n    description: NVIDIA H100 / H200 SXM5\n    name: xpu\n  - choice: switch\n    count: 4\n    description: NVIDIA NVSwitch\n    name: nvsw\n  - choice: switch\n    count: 3\n    description: Broadcom PCIe Gen5 Switch\n    name: pciesw\n  - choice: custom\n    count: 8\n    custom:\n      type: pcie_slot\n    description: PCIe Gen5 x16 slots (ConnectX / BlueField)\n    name: pciesl\n  - choice: device\n    count: 8\n    description: Mellanox ConnectX-5 100GbE NIC\n    name: cx5_100gbe\n  description: NVIDIA DGX System\n  edges:\n  - ep1:\n      component: cpu\n    ep2:\n      component: cpu\n    link: cpu_fabric\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesl[0:3]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[1]\n    ep2:\n      component: pciesl[4:7]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesw[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: cpu[1]\n    ep2:\n      component: pciesw[1]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[0]\n    ep2:\n      component: xpu[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[1]\n    ep2:\n      component: xpu[1]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[2]\n    ep2:\n      component: xpu[2]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[3]\n    ep2:\n      component: xpu[3]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[4]\n    ep2:\n      component: xpu[4]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[5]\n    ep2:\n      component: xpu[5]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[6]\n    ep2:\n      component: xpu[6]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[7]\n    ep2:\n      component: xpu[7]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: nvsw[0:4]\n    ep2:\n      component: pciesw[2]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesw[2]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: nvsw[0:4]\n    ep2:\n      component: xpu[0:8]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: pcie_endpoint[0]\n      device: cx5_100gbe[0:8]\n    ep2:\n      component: pciesl[0:8]\n    link: pcie\n    scheme: one2one\n  links:\n  - description: infinity_fabric\n    name: cpu_fabric\n  - description: nvlink_4\n    name: xpu_fabric\n  - description: PCI Express PCIE_GEN5 x16\n    name: pcie\n  name: dgx_h100\n- components:\n  - choice: custom\n    count: 1\n    custom:\n      type: pcie_endpoint\n    description: PCI Express GEN3 x16 endpoint\n    name: pcie_endpoint\n  - choice: cpu\n    count: 1\n    description: ConnectX-5 network processing ASIC\n    name: asic\n  - choice: port\n    count: 1\n    description: Ethernet port (100GbE)\n    name: port\n  - choice: device\n    count: 1\n    description: QSFP transceiver device\n    name: qsfp28\n  description: Mellanox ConnectX-5 100GbE NIC\n  edges:\n  - ep1:\n      component: pcie_endpoint\n    ep2:\n      component: asic\n    link: pcie_internal\n    scheme: one2one\n  - ep1:\n      component: asic\n    ep2:\n      component: port\n    link: serdes\n    scheme: many2many\n  - ep1:\n      component: port[0]\n    ep2:\n      component: electrical_port[0]\n      device: qsfp28[0]\n    link: electrical\n    scheme: one2one\n  - ep1:\n      component: port[0]\n    ep2:\n      component: electrical_port[0]\n      device: qsfp28[0]\n    link: electrical\n    scheme: one2one\n  links:\n  - description: Internal PCIe GEN3 fabric\n    name: pcie_internal\n  - description: High-speed SerDes lanes\n    name: serdes\n  - description: Electrical interface to transceiver\n    name: electrical\n  name: cx5_100gbe\n- components:\n  - choice: port\n    count: 1\n    description: Electrical host interface (4 lanes)\n    name: electrical_port\n  - choice: port\n    count: 1\n    description: Optical media interface (100Gbps)\n    name: optical_port\n  description: QSFP28 100Gbps Transceiver\n  edges:\n  - ep1:\n      component: electrical_port\n    ep2:\n      component: optical_port\n    link: internal_binding\n    scheme: one2one\n  links:\n  - description: Electrical-to-optical signal binding\n    name: internal_binding\n  name: qsfp28\ninstances:\n- count: 1\n  device: dgx_h100\n  name: dgx_h100\nname: dgx_cx5_qsfp_composed\n</code></pre>"},{"location":"blueprints/#example-dgx-with-cx5-as-a-nic","title":"Example: DGX with CX5 as a NIC","text":"<p>In some scenarios, a user may prefer to model <code>cx5</code> as an abstracted NIC within a <code>dgx</code> device rather than as a fully instantiated device.</p> <p>To add <code>cx5</code> as a <code>NIC</code> component, the user can initialize the <code>dgx</code> device by passing the CX5 variant directly, as shown below:</p> DGX with CX5 as NIC <pre><code>dgx_profile = \"dgx_h100\"\ncx5_variant = \"cx5_100g_single\"\ndevice = NvidiaDGX(dgx_profile, cx5_variant)\ninfrastructure = Api().infrastructure()\ninfrastructure.devices.append(device)\ninfrastructure.instances.add(name=device.name, device=device.name, count=1)\nservice = InfraGraphService()\nservice.set_graph(infrastructure)\ng = service.get_networkx_graph()\n</code></pre> <p>This creates a <code>cx5</code> as a nic component in <code>dgx</code> device. Generated YAML:</p> DGX with CX5 NIC definition as yaml <pre><code>devices:\n- components:\n  - choice: cpu\n    count: 2\n    description: AMD EPYC 9654 (Genoa)\n    name: cpu\n  - choice: xpu\n    count: 8\n    description: NVIDIA H100 / H200 SXM5\n    name: xpu\n  - choice: switch\n    count: 4\n    description: NVIDIA NVSwitch\n    name: nvsw\n  - choice: switch\n    count: 3\n    description: Broadcom PCIe Gen5 Switch\n    name: pciesw\n  - choice: custom\n    count: 8\n    custom:\n      type: pcie_slot\n    description: PCIe Gen5 x16 slots (ConnectX / BlueField)\n    name: pciesl\n  - choice: nic\n    count: 8\n    description: cx5_100g_single\n    name: cx5_100g_single\n  description: NVIDIA DGX System\n  edges:\n  - ep1:\n      component: cpu\n    ep2:\n      component: cpu\n    link: cpu_fabric\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesl[0:3]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[1]\n    ep2:\n      component: pciesl[4:7]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesw[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: cpu[1]\n    ep2:\n      component: pciesw[1]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[0]\n    ep2:\n      component: xpu[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[1]\n    ep2:\n      component: xpu[1]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[2]\n    ep2:\n      component: xpu[2]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[3]\n    ep2:\n      component: xpu[3]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[4]\n    ep2:\n      component: xpu[4]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[5]\n    ep2:\n      component: xpu[5]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[6]\n    ep2:\n      component: xpu[6]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[7]\n    ep2:\n      component: xpu[7]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: nvsw[0:4]\n    ep2:\n      component: pciesw[2]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesw[2]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: nvsw[0:4]\n    ep2:\n      component: xpu[0:8]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: pciesl[0]\n    ep2:\n      component: cx5_100g_single[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[1]\n    ep2:\n      component: cx5_100g_single[1]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[2]\n    ep2:\n      component: cx5_100g_single[2]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[3]\n    ep2:\n      component: cx5_100g_single[3]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[4]\n    ep2:\n      component: cx5_100g_single[4]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[5]\n    ep2:\n      component: cx5_100g_single[5]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[6]\n    ep2:\n      component: cx5_100g_single[6]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: pciesl[7]\n    ep2:\n      component: cx5_100g_single[7]\n    link: pcie\n    scheme: one2one\n  links:\n  - description: infinity_fabric\n    name: cpu_fabric\n  - description: nvlink_4\n    name: xpu_fabric\n  - description: PCI Express PCIE_GEN5 x16\n    name: pcie\n  name: dgx_h100\ninstances:\n- count: 1\n  device: dgx_h100\n  name: dgx_h100\nname: dgx_cx5_nic\n</code></pre>"},{"location":"blueprints/#fabric-blueprints","title":"Fabric Blueprints","text":"<p>Fabric blueprints allow users to define network fabric topologies by combining multiple devices and specifying their interconnections. They provide an intuitive way to model complex infrastructure setups such as datacenter tiers, clusters, or multi-device fabrics.</p> <p>All fabric blueprints are located in: <pre><code>src/infragraph/blueprints/fabrics/\n</code></pre> Please note that fabric builders only work with non-composed devices.</p>"},{"location":"blueprints/#creating-a-single-tier-fabric-with-multiple-dgx-hosts","title":"Creating a Single Tier Fabric with Multiple DGX Hosts","text":"<p>The following example demonstrates how to use the <code>SingleTierFabric</code> class to create a simple fabric connecting two DGX devices via a generic switch:</p> <pre><code>from infragraph.blueprints.devices.nvidia.dgx import NvidiaDGX\nfrom infragraph.blueprints.fabrics.single_tier_fabric import SingleTierFabric\n\nInstantiate a DGX device\ndgx = NvidiaDGX()\n\n# Create a single-tier fabric connecting two DGX devices via a single switch\nfabric = SingleTierFabric(dgx, 2) # 2 DGX devices\n# 'fabric' now contains the infrastructure graph with two DGX devices and the connecting switch\n</code></pre> <p>The <code>SingleTierFabric</code> blueprint returns an Infragraph object that includes two DGX devices along with a generic switch defined in the device blueprints, connected to form a simple topology.</p>"},{"location":"blueprints/#creating-a-clos-fat-tree-fabric-with-dgx-hosts","title":"Creating a CLOS Fat Tree Fabric with DGX Hosts","text":"<p>The following example demonstrates how to use the <code>ClosFatTreeFabric</code> class to create a clos fat tree fabric connecting dgx devices via a generic switch:</p> <pre><code>from infragraph.blueprints.fabrics.clos_fat_tree_fabric import ClosFatTreeFabric\nfrom infragraph.blueprints.devices.dgx import NvidiaDGX\nfrom infragraph.blueprints.devices.generic.generic_switch import Switch\n\n# Instantiate a DGX device\ndgx = NvidiaDGX()\n\n# Instantiate a Switch\nswitch = Switch(port_count=16)\n\n# Create a clos fat tree fabric with switch radix as 16 and with two levels:\nclos_fat_tree = ClosFatTreeFabric(switch, dgx, 2, [])\n# 'fabric' now contains the infrastructure graph with two tier clos fat tree fabric\n</code></pre>"},{"location":"blueprints/#clos-fat-tree-fabric-overview","title":"CLOS Fat Tree Fabric Overview","text":"<p>The CLOS Fat Tree fabric builds a scalable network using multiple levels of identical switches connected in a tree pattern. It is defined by FT(k, L), where: - k = number of ports per switch (switch radix) - L = number of levels in the fabric (depth)</p>"},{"location":"blueprints/#inputs","title":"Inputs","text":"<ul> <li>Switch device: Defines the switch type and port count (k).  </li> <li>Host device: Defines the servers or hosts (e.g., DGX) connected to the fabric edge.  </li> <li>Levels (L): Number of switch layers (tiers) in the network.  </li> <li>Bandwidth array: Link speeds at each level, e.g., host-to-edge (tier_0), edge-to-aggregation (tier_0 -&gt; tier_1), aggregation-to-spine (tier_1 -&gt; tier_2).</li> </ul>"},{"location":"blueprints/#two-level-fat-tree-network-sizing-computation","title":"Two-Level Fat-Tree: Network Sizing Computation","text":"<ol> <li>switch downlink: k/2 </li> <li>Number of Hosts:      = (2 * (switch_downlink) ^ Levels)/ (total ports in host)</li> <li>tier_0 (rack switches): Number of switches facing hosts.    = (2 * (switch_downlink) ^ Levels - 1)</li> <li>tier_1 (spine switches): Uplink switches connecting tier_0 switches.    = ((switch_downlink) ^ Levels - 1)</li> </ol> <p>Connections: Hosts \u2192 tier_0 (rack) \u2192 tier_1 (spines).</p>"},{"location":"blueprints/#three-level-fat-tree-topology-network-sizing-computation","title":"Three-Level Fat-Tree Topology: Network Sizing Computation","text":"<ol> <li>switch downlink: k/2 </li> <li>Number of Hosts:      = (2 * (switch_downlink) ^ Levels)/ (total ports in host)</li> <li>Tier_0 (rack switches):    = (2 * (switch_downlink) ^ Levels - 1)</li> <li>Tier_1 (aggregation switches): Same number as tier_0:    = (2 * (switch_downlink) ^ Levels - 1)</li> <li>Pods: Derived by dividing tier_0 by the downlinks per switch (k/2), matching pod size:    = switch downlink = k/2</li> <li>Spines (core switches): Number of spines is:    = ((switch_downlink) ^ Levels - 1)</li> <li>Spine Sets: Spine switches grouped to connect to tier_0 switches in pods.    = ((switch_downlink/2) * (tier_0 to tier_1 bandwidth)/(tier_1 to tier_2 bandwidth))</li> </ol> <p>Example FT(32, 2): - Switch Radix = 32 - Levels = 2 - Host per switch = 2 - Total Hosts = 64  - tier_0 switches = 32 - tier_1 switches = 16 - Pods = 2 - Spine sets = 16</p> <p>Connections: Hosts \u2192 tier_0 \u2192 tier_1 (spine switches).</p>"},{"location":"create/","title":"Case Study: Two Tier Clos Fabric","text":"<p>The main steps in designing a network infrastructure are as follows:</p> <ul> <li>Use text and/or diagrams to create an <code>infrastructure description</code></li> <li>Then <code>use the standardized schema to capture the infrastructure description</code> in a machine readable format by doing the following:<ul> <li>Add infrastructure <code>device</code> subgraphs using <code>components</code> and <code>links</code> to create <code>edges</code> between components</li> <li>Add infrastructure <code>instances</code> to define the number of devices in a reusable manner</li> <li>Add infrastructure <code>links</code> to define additional information that exists between device <code>instances</code> in the infrastructure</li> <li>Add infrastructure <code>edges</code> between device <code>instances</code></li> </ul> </li> </ul>"},{"location":"create/#infrastructure-description","title":"Infrastructure Description","text":"<p>The following is a diagrammatic and textual description of a <code>generic two tier clos fabric</code> that will be modeled using the standardized schema.</p> <p></p> <p>It consists of the following devices:</p> <ul> <li>4 generic <code>servers</code> with each server composed of 4 npus and 4 nics with each nic directly connected to one xpu via a pcie link.  Also every xpu in a server is connected to every other xpu by an nvlink switch. In addition the server includes a management nic that is separate from test nics.</li> <li>4 <code>leaf switches</code> composed of one asic and 16 ethernet ports</li> <li>3 <code>spine switches</code> composed of one asic and 16 ethernet ports</li> </ul> <p>The above devices will be interconnected in the following manner:</p> <ul> <li>each <code>leaf</code> switch is connected directly to 1 <code>server</code> and to all <code>spine</code> switches</li> <li>every <code>server</code> is connected to a <code>leaf</code> switch at 100 gpbs</li> <li>every <code>leaf</code> switch is connected to every <code>spine</code> switch at 400 gpbs</li> </ul>"},{"location":"create/#standardized-definitions","title":"Standardized Definitions","text":"<p>A standardized definition of the preceding two tier clos fabric can be created by following these steps:</p> <ul> <li>The device is a subgraph which is composed of two components connected to each other using a link.</li> <li>It acts as a blueprint allowing for a single definition to be reused multiple times for optimal space complexity.</li> </ul>"},{"location":"create/#create-a-server-device","title":"Create a Server Device","text":"<p>Define a server device based on the infrastructure description.</p> Server device definition using OpenApiArt generated classes <pre><code>from infragraph import *\n\n# pyright: reportArgumentType=false\n\n\nclass Server(Device):\n    def __init__(self, npu_factor: int = 1):\n        \"\"\"Adds an InfraGraph device to infrastructure based on the following components:\n        - 1 cpu for every 2 npus\n        - 1 pcie switch for every 1 cpu\n        - X npus = npu_factor * 2\n        - 1 nic for every xpu with 2 nics connected to a pcie switch\n        - 1 nvswitch connected to all npus\n        \"\"\"\n        super(Device, self).__init__()\n        self.name = \"server\"\n        self.description = \"A generic server with npu_factor * 4 xpu(s)\"\n\n        cpu = self.components.add(\n            name=\"cpu\",\n            description=\"Generic CPU\",\n            count=npu_factor,\n        )\n        cpu.choice = Component.CPU\n        xpu = self.components.add(\n            name=\"xpu\",\n            description=\"Generic GPU/XPU\",\n            count=npu_factor * 2,\n        )\n        xpu.choice = Component.XPU\n        nvlsw = self.components.add(\n            name=\"nvlsw\",\n            description=\"NVLink Switch\",\n            count=1,\n        )\n        nvlsw.choice = Component.SWITCH\n        pciesw = self.components.add(\n            name=\"pciesw\",\n            description=\"PCI Express Switch Gen 4\",\n            count=npu_factor,\n        )\n        pciesw.choice = Component.SWITCH\n        nic = self.components.add(\n            name=\"nic\",\n            description=\"Generic Nic\",\n            count=npu_factor * 2,\n        )\n        nic.choice = Component.NIC\n        mgmt = self.components.add(\n            name=\"mgmt\",\n            description=\"Mgmt Nic\",\n            count=1,\n        )\n        mgmt.custom.type = \"mgmt-nic\"\n\n        cpu_fabric = self.links.add(name=\"fabric\", description=\"CPU Fabric\")\n        nvlink = self.links.add(name=\"nvlink\")\n        pcie = self.links.add(name=\"pcie\")\n\n        edge = self.edges.add(scheme=DeviceEdge.ONE2ONE, link=pcie.name)\n        edge.ep1.component = mgmt.name\n        edge.ep2.component = f\"{cpu.name}[0]\"\n\n        edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=cpu_fabric.name)\n        edge.ep1.component = cpu.name\n        edge.ep2.component = cpu.name\n\n        edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=nvlink.name)\n        edge.ep1.component = xpu.name\n        edge.ep2.component = nvlsw.name\n\n        for idx in range(pciesw.count):\n            edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=pcie.name)\n            edge.ep1.component = f\"{cpu.name}[{idx}]\"\n            edge.ep2.component = f\"{pciesw.name}[{idx}]\"\n\n        npu_slices = [f\"{idx}:{idx+2}\" for idx in range(0, xpu.count, 2)]\n        for npu_idx, pciesw_idx in zip(npu_slices, range(pciesw.count)):\n            edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=pcie.name)\n            edge.ep1.component = f\"{xpu.name}[{npu_idx}]\"\n            edge.ep2.component = f\"{pciesw.name}[{pciesw_idx}]\"\n\n        for nic_idx, pciesw_idx in zip(npu_slices, range(pciesw.count)):\n            edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=pcie.name)\n            edge.ep1.component = f\"{nic.name}[{nic_idx}]\"\n            edge.ep2.component = f\"{pciesw.name}[{pciesw_idx}]\"\n\n\nif __name__ == \"__main__\":\n    device = Server(npu_factor=2)\n    device.validate()\n    print(device.serialize(encoding=Device.YAML))\n</code></pre> Server device definition as yaml <pre><code>components:\n- choice: cpu\n  count: 1\n  description: Generic CPU\n  name: cpu\n- choice: xpu\n  count: 2\n  description: Generic GPU/XPU\n  name: xpu\n- choice: switch\n  count: 1\n  description: NVLink Switch\n  name: nvlsw\n- choice: switch\n  count: 1\n  description: PCI Express Switch Gen 4\n  name: pciesw\n- choice: nic\n  count: 2\n  description: Generic Nic\n  name: nic\n- choice: custom\n  count: 1\n  custom:\n    type: mgmt-nic\n  description: Mgmt Nic\n  name: mgmt\ndescription: A generic server with npu_factor * 4 xpu(s)\nedges:\n- ep1:\n    component: mgmt\n  ep2:\n    component: cpu[0]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: cpu\n  ep2:\n    component: cpu\n  link: fabric\n  scheme: many2many\n- ep1:\n    component: xpu\n  ep2:\n    component: nvlsw\n  link: nvlink\n  scheme: many2many\n- ep1:\n    component: cpu[0]\n  ep2:\n    component: pciesw[0]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: xpu[0:2]\n  ep2:\n    component: pciesw[0]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: nic[0:2]\n  ep2:\n    component: pciesw[0]\n  link: pcie\n  scheme: many2many\nlinks:\n- description: CPU Fabric\n  name: fabric\n- name: nvlink\n- name: pcie\nname: server\n</code></pre>"},{"location":"create/#create-a-switch-device","title":"Create a Switch Device","text":"<p>Define a switch device based on the infrastructure description.</p> Switch device definition using OpenApiArt generated classes <pre><code>from infragraph import *\n\n# pyright: reportArgumentType=false\n\n\nclass Switch(Device):\n    def __init__(self, port_count: int = 16):\n        \"\"\"Adds an InfraGraph device to infrastructure based on the following components:\n        - 1 generic asic\n        - nic_count number of ports\n        - integrated circuitry connecting ports to asic\n        \"\"\"\n        super(Device, self).__init__()\n        self.name = \"switch\"\n        self.description = \"A generic switch\"\n\n        asic = self.components.add(\n            name=\"asic\",\n            description=\"Generic ASIC\",\n            count=1,\n        )\n        asic.choice = Component.CPU\n        port = self.components.add(\n            name=\"port\",\n            description=\"Generic port\",\n            count=port_count,\n        )\n        port.choice = Component.PORT\n\n        ic = self.links.add(name=\"ic\", description=\"Generic integrated circuitry\")\n\n        edge = self.edges.add(scheme=DeviceEdge.MANY2MANY, link=ic.name)\n        edge.ep1.component = asic.name\n        edge.ep2.component = port.name\n\n\nif __name__ == \"__main__\":\n    device = Switch()\n    print(device.serialize(encoding=Device.YAML))\n</code></pre> Switch device definition as yaml <pre><code>components:\n- choice: cpu\n  count: 1\n  description: Generic ASIC\n  name: asic\n- choice: port\n  count: 16\n  description: Generic port\n  name: port\ndescription: A generic switch\nedges:\n- ep1:\n    component: asic\n  ep2:\n    component: port\n  link: ic\n  scheme: many2many\nlinks:\n- description: Generic integrated circuitry\n  name: ic\nname: switch\n</code></pre>"},{"location":"create/#create-an-infrastructure-of-instances-of-devices-links-and-edges","title":"Create an Infrastructure of Instances of devices, Links and Edges","text":"<p>Define an infrastructure based on the infrastructure description.</p> Two Tier Clos Fabric Infrastructure using OpenApiArt generated classes <pre><code>from infragraph import *\nfrom infragraph.blueprints.devices.generic.server import Server\nfrom infragraph.blueprints.devices.generic.generic_switch import Switch\nfrom infragraph.infragraph_service import InfraGraphService\n\n\nclass ClosFabric(Infrastructure):\n    \"\"\"Return a 2 tier clos fabric with the following characteristics:\n    - 4 generic servers\n    - each generic server with 2 npus and 2 nics\n    - 4 leaf switches each with 16 ports\n    - 3 spine switch each with 16 ports\n    - connectivity between servers and leaf switches is 100G\n    - connectivity between servers and spine switch is 400G\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(name=\"closfabric\", description=\"2 Tier Clos Fabric\")\n\n        server = Server()\n        switch = Switch()\n        self.devices.append(server).append(switch)\n\n        hosts = self.instances.add(name=\"host\", device=server.name, count=4)\n        leaf_switches = self.instances.add(name=\"leafsw\", device=switch.name, count=4)\n        spine_switches = self.instances.add(name=\"spinesw\", device=switch.name, count=3)\n\n        leaf_link = self.links.add(\n            name=\"leaf-link\",\n            description=\"Link characteristics for connectivity between servers and leaf switches\",\n        )\n        leaf_link.physical.bandwidth.gigabits_per_second = 100\n        spine_link = self.links.add(\n            name=\"spine-link\",\n            description=\"Link characteristics for connectivity between leaf switches and spine switches\",\n        )\n        spine_link.physical.bandwidth.gigabits_per_second = 400\n\n        host_component = InfraGraphService.get_component(server, Component.NIC)\n        switch_component = InfraGraphService.get_component(switch, Component.PORT)\n\n        # link each host to one leaf switch\n        for idx in range(hosts.count):\n            edge = self.edges.add(scheme=InfrastructureEdge.ONE2ONE, link=leaf_link.name)\n            edge.ep1.instance = f\"{hosts.name}[{idx}]\"\n            edge.ep1.component = host_component.name\n            edge.ep2.instance = f\"{leaf_switches.name}[{idx}]\"\n            edge.ep2.component = switch_component.name\n\n        # link every leaf switch to every spine switch\n        print()\n        for leaf_idx in range(leaf_switches.count):\n            for spine_idx in range(spine_switches.count):\n                edge = self.edges.add(scheme=InfrastructureEdge.ONE2ONE, link=spine_link.name)\n                edge.ep1.instance = f\"{leaf_switches.name}[{leaf_idx}]\"\n                edge.ep1.component = f\"{switch_component.name}[{host_component.count + spine_idx}]\"\n                edge.ep2.instance = f\"{spine_switches.name}[{spine_idx}]\"\n                edge.ep2.component = f\"{switch_component.name}[{leaf_idx}]\"\n</code></pre> ClosFabric infrastructure definition as yaml <pre><code>description: 2 Tier Clos Fabric\ndevices:\n- components:\n  - choice: cpu\n    count: 1\n    description: Generic CPU\n    name: cpu\n  - choice: xpu\n    count: 2\n    description: Generic GPU/XPU\n    name: xpu\n  - choice: switch\n    count: 1\n    description: NVLink Switch\n    name: nvlsw\n  - choice: switch\n    count: 1\n    description: PCI Express Switch Gen 4\n    name: pciesw\n  - choice: nic\n    count: 2\n    description: Generic Nic\n    name: nic\n  - choice: custom\n    count: 1\n    custom:\n      type: mgmt-nic\n    description: Mgmt Nic\n    name: mgmt\n  description: A generic server with npu_factor * 4 xpu(s)\n  edges:\n  - ep1:\n      component: mgmt\n    ep2:\n      component: cpu[0]\n    link: pcie\n    scheme: one2one\n  - ep1:\n      component: cpu\n    ep2:\n      component: cpu\n    link: fabric\n    scheme: many2many\n  - ep1:\n      component: xpu\n    ep2:\n      component: nvlsw\n    link: nvlink\n    scheme: many2many\n  - ep1:\n      component: cpu[0]\n    ep2:\n      component: pciesw[0]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: xpu[0:2]\n    ep2:\n      component: pciesw[0]\n    link: pcie\n    scheme: many2many\n  - ep1:\n      component: nic[0:2]\n    ep2:\n      component: pciesw[0]\n    link: pcie\n    scheme: many2many\n  links:\n  - description: CPU Fabric\n    name: fabric\n  - name: nvlink\n  - name: pcie\n  name: server\n- components:\n  - choice: cpu\n    count: 1\n    description: Generic ASIC\n    name: asic\n  - choice: port\n    count: 16\n    description: Generic port\n    name: port\n  description: A generic switch\n  edges:\n  - ep1:\n      component: asic\n    ep2:\n      component: port\n    link: ic\n    scheme: many2many\n  links:\n  - description: Generic integrated circuitry\n    name: ic\n  name: switch\nedges:\n- ep1:\n    component: nic\n    instance: host[0]\n  ep2:\n    component: port\n    instance: leafsw[0]\n  link: leaf-link\n  scheme: one2one\n- ep1:\n    component: nic\n    instance: host[1]\n  ep2:\n    component: port\n    instance: leafsw[1]\n  link: leaf-link\n  scheme: one2one\n- ep1:\n    component: nic\n    instance: host[2]\n  ep2:\n    component: port\n    instance: leafsw[2]\n  link: leaf-link\n  scheme: one2one\n- ep1:\n    component: nic\n    instance: host[3]\n  ep2:\n    component: port\n    instance: leafsw[3]\n  link: leaf-link\n  scheme: one2one\n- ep1:\n    component: port[2]\n    instance: leafsw[0]\n  ep2:\n    component: port[0]\n    instance: spinesw[0]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[3]\n    instance: leafsw[0]\n  ep2:\n    component: port[0]\n    instance: spinesw[1]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[4]\n    instance: leafsw[0]\n  ep2:\n    component: port[0]\n    instance: spinesw[2]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[2]\n    instance: leafsw[1]\n  ep2:\n    component: port[1]\n    instance: spinesw[0]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[3]\n    instance: leafsw[1]\n  ep2:\n    component: port[1]\n    instance: spinesw[1]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[4]\n    instance: leafsw[1]\n  ep2:\n    component: port[1]\n    instance: spinesw[2]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[2]\n    instance: leafsw[2]\n  ep2:\n    component: port[2]\n    instance: spinesw[0]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[3]\n    instance: leafsw[2]\n  ep2:\n    component: port[2]\n    instance: spinesw[1]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[4]\n    instance: leafsw[2]\n  ep2:\n    component: port[2]\n    instance: spinesw[2]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[2]\n    instance: leafsw[3]\n  ep2:\n    component: port[3]\n    instance: spinesw[0]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[3]\n    instance: leafsw[3]\n  ep2:\n    component: port[3]\n    instance: spinesw[1]\n  link: spine-link\n  scheme: one2one\n- ep1:\n    component: port[4]\n    instance: leafsw[3]\n  ep2:\n    component: port[3]\n    instance: spinesw[2]\n  link: spine-link\n  scheme: one2one\ninstances:\n- count: 4\n  device: server\n  name: host\n- count: 4\n  device: switch\n  name: leafsw\n- count: 3\n  device: switch\n  name: spinesw\nlinks:\n- description: Link characteristics for connectivity between servers and leaf switches\n  name: leaf-link\n  physical:\n    bandwidth:\n      choice: gigabits_per_second\n      gigabits_per_second: 100\n- description: Link characteristics for connectivity between leaf switches and spine\n    switches\n  name: spine-link\n  physical:\n    bandwidth:\n      choice: gigabits_per_second\n      gigabits_per_second: 400\nname: closfabric\n</code></pre>"},{"location":"ecosystem/","title":"Chakra + InfraGraph Ecosystem","text":"<p>MLCommons Chakra represents the details of any AI workload by capturing Execution Traces (ET) - graphs of operators, tensors, dependencies, and timing.</p> <p>InfraGraph represents the underlying systems and infrastructure used for AI training or inference - hosts, NICs, xPUs/accelerators, interconnects, and topologies - using programmatic system blueprints.</p> <p>Together, Chakra + InfraGraph let you pair workload traces with infrastructure blueprints to analyze current systems and co\u2011design future ones, while safely sharing artifacts across teams and partners.</p> <p></p>"},{"location":"ecosystem/#infragraph-helper-tools","title":"InfraGraph Helper Tools","text":"<p>Like Chakra, InfraGraph also proposes a set of helper tools that can help users while working with InfraGraph. </p>"},{"location":"ecosystem/#1-converters","title":"1. Converters","text":"<p>Many open\u2011source and commercial tools model systems using their own schemas (e.g., lspci, lshw, OpenMPI tools, NetBox, etc.). InfraGraph is adding translators to ensure interoperability - import from (and export to) these formats without rewriting everything.</p>"},{"location":"ecosystem/#2-discoverers","title":"2. Discoverers","text":"<p>Discovery has two aspects:</p> <ul> <li>Configuration discovery - capture the attributes of each node/host device.</li> <li>Topology discovery - map interconnections across a distributed system.</li> </ul> <p>InfraGraph aims to support dynamic discovery so infrastructure details can be captured automatically and kept up to date.</p>"},{"location":"ecosystem/#3-blueprints","title":"3. Blueprints","text":"<p>We\u2019re building blueprint templates for commonly used systems in AI data centers (AI DCs):</p> <ul> <li>Devices - hosts from NVIDIA, Meta, Dell, HPE, etc. vendors, devices like NICs, XPUs/accelerators, and other components.</li> <li>Fabrics/topologies - standard definitions such as Clos and Dragonfly.</li> </ul> <p>These blueprints help researchers quickly assemble infrastructure definitions for experimentation and serve as reference models when building new designs.</p>"},{"location":"ecosystem/#4-visualizers","title":"4. Visualizers","text":"<p>Graphical views make it easier to spot design issues:</p> <ul> <li>Drill\u2011down to component\u2011level details.</li> <li>Zoom\u2011out to high\u2011level system connectivity.</li> </ul> <p>Visualizers assist developers in understanding structure, bottlenecks, and correctness.</p> <p>Note: Some of these tools are work-in-progress and we invite contributions from the community.</p>"},{"location":"ecosystem/#privacy-obfuscation","title":"Privacy &amp; Obfuscation","text":"<p>InfraGraph, like Chakra, supports selective disclosure and obfuscation so you can safely share only high\u2011level infrastructure definitions.</p>"},{"location":"ecosystem/#private-annotations","title":"Private annotations:","text":"<p>One can attach additional details to infrastructure elements or devices, links, fabrics, racks/pods - as annotations (structured fields or free text). These may include vendor\u2011specific information, firmware/driver versions, internal asset tags, SKUs, or configuration notes. </p> <ul> <li>Annotations can be kept private for internal tools and workflows. </li> <li>Private annotations can be excluded or obfuscated when exporting or sharing the InfraGraph outside your organization.</li> </ul>"},{"location":"ecosystem/#safe-sharing","title":"Safe sharing:","text":"<p>When publishing to vendors or the open\u2011source community, you decide which fields remain, which are transformed, and which are removed - preserving the high\u2011level system definition while protecting sensitive details about the network and device configurations.</p>"},{"location":"ecosystem/#sharing-interoperability","title":"Sharing &amp; Interoperability","text":"<p>InfraGraph definitions can be shared with vendors and the open\u2011source community and used with a variety of analyzers, simulators, or emulators (open or commercial). The emphasis is on:</p> <ul> <li>Portability: consistent, schema\u2011driven artifacts.</li> <li>Compatibility: converters for common ecosystem tools.</li> <li>Safety: optional obfuscation to protect IP while enabling collaboration.</li> <li>Composability: components or devices defined separately can be reused across systems or inserted into different topologies, making it easy to mix, match, and assemble infrastructure from modular parts.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>These examples demonstrate <code>describing</code> a variety of devices and infrastructures using text descriptions and diagrams to <code>defining</code> them using a <code>standardized schema</code>.</p>"},{"location":"examples/#dgx-a100-server","title":"DGX-A100 Server","text":"<p>This server diagram acts as an example of how multiple components can be connected to a single component such as multiple gpu components are connected to a single pcie switch.</p> <p>The graph model is able to capture the asymmetric layout of the device.</p>"},{"location":"examples/#description","title":"Description","text":""},{"location":"examples/#standardized-definition","title":"Standardized Definition","text":"DGX device definition using OpenApiArt generated classes <pre><code>from typing import Optional, Literal, Dict, Union\nfrom infragraph import *\n\nDgxProfile = Literal[\n    \"dgx1\",\n    \"dgx2\",\n    \"dgx_a100\",\n    \"dgx_h100\",\n    # \"dgx_gh200\",\n    \"dgx_gb200\",\n]\n\nNicSpec = Union[str, Device]\n\nDGX_PROFILE_CATALOG: Dict[DgxProfile, dict] = {\n\n    \"dgx1\": {\n        \"cpu\": {\n            \"description\": \"Intel Xeon E5-2698 v4 (Broadwell-EP)\",\n            \"fabric\": \"qpi\",\n            \"count\": 2,\n        },\n        \"xpu\": {\n            \"description\": \"NVIDIA Tesla P100 SXM2\",\n            \"fabric\": \"nvlink_1\",\n            \"count\": 8,\n            \"nvswitch_count\": 0,\n        },\n        \"pciesw\": {\n            \"description\": \"PLX PEX8796 PCIe Switch\",\n            \"fabric\": \"pcie_gen3\",\n            \"count\": 4,   # one per GPU pair\n        },\n        \"pciesl\": {\n            \"description\": \"Internal PCIe x16 endpoints (GPU-attached)\",\n            \"fabric\": \"pcie_gen3\",\n            \"count\": 4,\n        }\n    },\n\n    \"dgx2\": {\n        \"cpu\": {\n            \"description\": \"Intel Xeon Platinum 8168 (Skylake-SP)\",\n            \"fabric\": \"upi\",\n            \"count\": 2,\n        },\n        \"xpu\": {\n            \"description\": \"NVIDIA Tesla V100 SXM2\",\n            \"fabric\": \"nvlink_2\",\n            \"count\": 16,\n            \"nvswitch_count\": 12,\n        },\n        \"pciesw\": {\n            \"description\": \"Broadcom / PLX PEX8796 PCIe Switch\",\n            \"fabric\": \"pcie_gen3\",\n            \"count\": 14,\n        },\n        \"pciesl\": {\n            \"description\": \"Internal PCIe x16 endpoints (GPU + NVSwitch)\",\n            \"fabric\": \"pcie_gen3\",\n            \"count\": 8,\n        }\n    },\n\n    \"dgx_a100\": {\n        \"cpu\": {\n            \"description\": \"AMD EPYC 7742 (Rome)\",\n            \"fabric\": \"infinity_fabric\",\n            \"count\": 2,\n        },\n        \"xpu\": {\n            \"description\": \"NVIDIA A100 SXM4\",\n            \"fabric\": \"nvlink_3\",\n            \"count\": 8,\n            \"nvswitch_count\": 6,\n        },\n        \"pciesw\": {\n            \"description\": \"Broadcom / PLX PCIe Gen4 Switch\",\n            \"fabric\": \"pcie_gen4\",\n            \"count\": 5,\n        },\n        \"pciesl\": {\n            \"description\": \"PCIe Gen4 x16 slots (NIC / storage)\",\n            \"fabric\": \"pcie_gen4\",\n            \"count\": 8,\n        }\n    },\n\n    \"dgx_h100\": {\n        \"cpu\": {\n            \"description\": \"AMD EPYC 9654 (Genoa)\",\n            \"fabric\": \"infinity_fabric\",\n            \"count\": 2,\n        },\n        \"xpu\": {\n            \"description\": \"NVIDIA H100 / H200 SXM5\",\n            \"fabric\": \"nvlink_4\",\n            \"count\": 8,\n            \"nvswitch_count\": 4,\n        },\n        \"pciesw\": {\n            \"description\": \"Broadcom PCIe Gen5 Switch\",\n            \"fabric\": \"pcie_gen5\",\n            \"count\": 3,\n        },\n        \"pciesl\": {\n            \"description\": \"PCIe Gen5 x16 slots (ConnectX / BlueField)\",\n            \"fabric\": \"pcie_gen5\",\n            \"count\": 8,\n        }\n    },\n\n    # \"dgx_gh200\": {\n    #     \"cpu\": {\n    #         \"description\": \"NVIDIA Grace CPU\",\n    #         \"fabric\": \"nvlink_c2c\",\n    #         \"count\": 2,\n    #     },\n    #     \"xpu\": {\n    #         \"description\": \"NVIDIA Grace Hopper Superchip\",\n    #         \"fabric\": \"nvlink_c2c\",\n    #         \"count\": 4,\n    #         \"nvswitch_count\": 0,\n    #     },\n    #     \"pciesw\": {\n    #         \"description\": \"No discrete PCIe switches (NVLink-C2C system)\",\n    #         \"fabric\": None,\n    #         \"count\": 0,\n    #     },\n    #     \"pciesl\": {\n    #         \"description\": \"External PCIe via Grace IO die\",\n    #         \"fabric\": \"pcie_gen5\",\n    #         \"count\": 4,\n    #     }\n    # },\n\n    \"dgx_gb200\": {\n        \"cpu\": {\n            \"description\": \"NVIDIA Grace CPU Superchip\",\n            \"fabric\": \"nvlink_c2c\",\n            \"count\": 2,\n        },\n        \"xpu\": {\n            \"description\": \"NVIDIA Grace Blackwell Superchip\",\n            \"fabric\": \"nvlink_c2c\",\n            \"count\": 4,\n            \"nvswitch_count\": 18,\n        },\n        \"pciesw\": {\n            \"description\": \"\",\n            \"fabric\": \"pcie_gen5\",\n            \"count\": 0,\n        },\n        \"pciesl\": {\n            \"description\": \"External PCIe Gen5 lanes from Grace\",\n            \"fabric\": \"pcie_gen5\",\n            \"count\": 4,\n        }\n    },\n}\n\n\nclass NvidiaDGX(Device):\n    \"\"\"\n    InfraGraph model of an NVIDIA DGX system.\n\n    This class represents a *profile-locked* DGX platform (DGX-1 through\n    DGX GB200) using an authoritative hardware catalog. Each profile defines\n    a fixed topology for CPUs, XPUs (GPUs or superchips), NVLink fabric,\n    optional NVSwitches, PCIe switches, slots, and network interfacee.\n\n    The model supports both x86-based DGX systems and Grace / Grace-Hopper /\n    Grace-Blackwell architectures, automatically adapting the fabric and\n    PCIe topology based on the selected profile.\n\n    Key features:\n    - Profile-locked CPU/XPU/NVSwitch combinations\n    - Explicit modeling of NVLink, NVLink-C2C, and PCIe fabrics\n    - Optional PCIe topology for x86 DGX systems only\n    - Flexible NIC modeling via Union[str, Device]\n    - Modular internal construction for easy extension and validation\n\n    This class is intended for architectural modeling, topology validation,\n    and infrastructure visualization rather than runtime configuration.\n    \"\"\"\n    def __init__(\n        self,\n        profile: DgxProfile = \"dgx_h100\",\n        nic_device: Optional[NicSpec] = None,\n    ):\n        \"\"\"\n        Initialize an NVIDIA DGX system model.\n\n        Args:\n            profile (DgxProfile, optional):\n                DGX system profile to instantiate. The profile fully determines\n                the CPU, XPU, NVLink generation, NVSwitch count, and PCIe\n                topology. Defaults to \"dgx_h100\".\n\n            nic_device (Optional[Union[str, Device]], optional):\n                Network interface specification for PCIe-attached NICe.\n\n                Supported values:\n                - None:\n                    Adds generic NIC components, one per PCIe slot.\n                - str:\n                    Uses the provided string as a symbolic NIC description\n                    (e.g. \"ConnectX-7 400Gb\"), applied to all NIC component.\n                - Device:\n                    Attaches a fully composed InfraGraph Device representing\n                    a NIC or DPU, connected via its PCIe endpoint.\n\n                Defaults to None.\n\n        Raises:\n            ValueError:\n                If an unsupported DGX profile is specified.\n\n            TypeError:\n                If nic_device is not None, str, or Device.\n        \"\"\"\n        super(Device, self).__init__()\n\n        if profile not in DGX_PROFILE_CATALOG:\n            raise ValueError(f\"Unsupported DGX profile: {profile}\")\n\n        self.profile = profile\n        self.catalog = DGX_PROFILE_CATALOG[profile]\n\n        self.name = profile\n        self.description = \"NVIDIA DGX System\"\n\n        self.cpu = self._add_cpu()\n        self.xpu = self._add_xpu()\n        self.nvsw = self._add_nvswitch()\n        self.pciesw, self.pciesl = self._add_pcie_components()\n\n        self._add_links()\n        self._wire_cpu()\n        self._wire_pcie()\n        self._wire_xpu()\n        self._add_nics(nic_device)\n\n    # Components\n\n    def _add_cpu(self):\n        cfg = self.catalog[\"cpu\"]\n        cpu = self.components.add(\n            name=\"cpu\",\n            description=cfg[\"description\"],\n            count=cfg[\"count\"],\n        )\n        cpu.choice = Component.CPU\n        return cpu\n\n    def _add_xpu(self):\n        cfg = self.catalog[\"xpu\"]\n        xpu = self.components.add(\n            name=\"xpu\",\n            description=cfg[\"description\"],\n            count=cfg[\"count\"],\n        )\n        xpu.choice = Component.XPU\n        return xpu\n\n    def _add_nvswitch(self):\n        count = self.catalog[\"xpu\"][\"nvswitch_count\"]\n        if count &lt;= 0:\n            return None\n        sw = self.components.add(\n            name=\"nvsw\",\n            description=\"NVIDIA NVSwitch\",\n            count=count,\n        )\n        sw.choice = Component.SWITCH\n        return sw\n\n    def _add_pcie_components(self):\n        pciesw = self.catalog[\"pciesw\"]\n        pciesl = self.catalog[\"pciesl\"]\n\n        if pciesw[\"count\"] &gt; 0:\n            sw = self.components.add(\n                name=\"pciesw\",\n                description=pciesw[\"description\"],\n                count=pciesw[\"count\"],\n            )\n            sw.choice = Component.SWITCH\n        else:\n            sw = None\n\n        if pciesl[\"count\"] &gt; 0:\n            sl = self.components.add(\n                name=\"pciesl\",\n                description=pciesl[\"description\"],\n                count=pciesl[\"count\"],\n            )\n            sl.choice = Component.CUSTOM\n            sl.custom.type = \"pcie_slot\"\n        else:\n            sl = None\n\n        return sw, sl\n\n    # Links\n    def _add_links(self):\n        cpu_cfg = self.catalog[\"cpu\"]\n        xpu_cfg = self.catalog[\"xpu\"]\n\n        self.cpu_fabric = self.links.add(\n            name=\"cpu_fabric\",\n            description=cpu_cfg[\"fabric\"],\n        )\n\n        self.xpu_fabric = self.links.add(\n            name=\"xpu_fabric\",\n            description=xpu_cfg[\"fabric\"],\n        )\n\n        if self.catalog['pciesw']['fabric'] is not None:\n            self.pcie = self.links.add(\n                \"pcie\", f\"PCI Express {self.catalog['pciesw']['fabric'].upper()} x16\"\n            )\n\n        if self.profile == \"dgx_gb200\":\n            self.c2c_fabric = self.links.add(\n            name=\"c2c_link\",\n            description=\"\",\n            )\n\n\n    # Wiring\n    def _wire_cpu(self):\n        edge = self.edges.add(DeviceEdge.MANY2MANY, self.cpu_fabric.name)\n        edge.ep1.component = \"cpu\"\n        edge.ep2.component = \"cpu\"\n\n    def _wire_pcie(self):\n        if self.profile == \"dgx1\":\n            pciesw_to_xpu_mapping = {\n                0: [0, 2],\n                1: [1, 3],\n                2: [4, 6],\n                3: [5, 7],\n            }\n            for sw, gpus in pciesw_to_xpu_mapping.items():\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{sw}]\"\n                e.ep2.component = f\"xpu[{gpus[0]}:{gpus[1]+1}:2]\"\n\n            for sw in range(0, self.pciesw.count):\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{sw}]\"\n                e.ep2.component = f\"{self.pciesl.name}[{sw}]\"\n\n            for cpu_index in range(0, self.cpu.count):\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{ 2 * cpu_index }]\"\n                e.ep2.component = f\"{self.cpu.name}[{cpu_index}]\"\n\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{ 2  * cpu_index + 1}]\"\n                e.ep2.component = f\"{self.cpu.name}[{cpu_index}]\"\n            return\n\n        if self.profile == \"dgx2\":\n            # connect pciesw to v100s\n            for pciesw_index in range(0, 8):\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{pciesw_index}]\"\n                e.ep2.component = f\"{self.xpu.name}[{2*pciesw_index}:{2*pciesw_index + 2}]\"\n\n            # connect slots to pciesw\n            for pciesw_index in range(0, 8):\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{pciesw_index}]\"\n                e.ep2.component = f\"{self.pciesl.name}[{pciesw_index}]\"  \n\n            # connect root pciesw to cpu\n            pairs = [\n                (\"8:10\", 0),\n                (\"10:12\", 1),\n            ]\n\n            for sw_range, cpu_idx in pairs:\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{sw_range}]\"\n                e.ep2.component = f\"{self.cpu.name}[{cpu_idx}]\"\n\n            pairs = [\n                (8, \"0:2\"),\n                (9, \"2:4\"),\n                (10, \"4:6\"),\n                (11, \"6:8\"),\n            ]\n\n            for root_pcie_sw_idx, pcie_idx in pairs:\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{root_pcie_sw_idx}]\"\n                e.ep2.component = f\"{self.pciesw.name}[{pcie_idx}]\"\n\n            return\n\n        if self.profile == \"dgx_a100\":\n            # cpu to pciesw\n            for cpu_idx, sw_idxs in [(0, [0, 1]), (1, [2, 3])]:\n                for sw in sw_idxs:\n                    e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                    e.ep1.component = f\"{self.cpu.name}[{cpu_idx}]\"\n                    e.ep2.component = f\"{self.pciesw.name}[{sw}]\"\n\n            # pciesw to pciesl and xpu\n            for pciesw_index in range(0, 4):\n                # pciesw to pciesl\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{pciesw_index}]\"\n                e.ep2.component = f\"{self.pciesl.name}[{2 * pciesw_index}:{2 * pciesw_index + 2}]\"\n\n                # pciesw to xpu\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.pciesw.name}[{pciesw_index}]\"\n                e.ep2.component = f\"{self.xpu.name}[{2 * pciesw_index}:{2 * pciesw_index + 2}]\"\n\n            # nvsw to pciesw\n            e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n            e.ep1.component = f\"{self.pciesw.name}[4]\"\n            e.ep2.component = f\"{self.nvsw.name}[0:6]\"   \n\n            # pciesw  4 - 3\n            e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n            e.ep1.component = f\"{self.pciesw.name}[4]\"\n            e.ep2.component = f\"{self.pciesw.name}[3]\"\n            return\n\n        if self.profile == \"dgx_h100\":\n            for cpu_idx in range(0, 2):\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.cpu.name}[{cpu_idx}]\"\n                e.ep2.component = f\"{self.pciesl.name}[{cpu_idx*4}:{cpu_idx*4 + 3}]\"\n\n            for cpu_idx in range(0, 2):\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.cpu.name}[{cpu_idx}]\"\n                e.ep2.component = f\"{self.pciesw.name}[{cpu_idx}]\"\n\n            for pciesl_idx in range(0, self.pciesl.count):\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n                e.ep1.component = f\"{self.pciesl.name}[{pciesl_idx}]\"\n                e.ep2.component = f\"{self.xpu.name}[{pciesl_idx}]\"\n\n            e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n            e.ep1.component = f\"{self.nvsw.name}[0:4]\"\n            e.ep2.component = f\"{self.pciesw.name}[2]\"\n\n            e = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n            e.ep1.component = f\"{self.cpu.name}[0]\"\n            e.ep2.component = f\"{self.pciesw.name}[2]\"\n            return\n\n        if self.profile == \"dgx_gb200\":\n            # connect pcie and c2c\n            for cpu_idx in range(0, 2):\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n                e.ep1.component = f\"{self.cpu.name}[{cpu_idx}]\"\n                e.ep2.component = f\"{self.pciesl.name}[{2*cpu_idx}:{2*cpu_idx + 2}]\"\n\n            for cpu_idx in range(0, 2):\n                e = self.edges.add(DeviceEdge.MANY2MANY, self.c2c_fabric.name)\n                e.ep1.component = f\"{self.cpu.name}[{cpu_idx}]\"\n                e.ep2.component = f\"{self.xpu.name}[{2*cpu_idx}:{2*cpu_idx + 2}]\"\n\n            return\n\n    def _wire_xpu(self):\n        if self.profile == \"dgx1\":\n            # clique 0\u20133\n            e1 = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e1.ep1.component = \"xpu[0:4]\"\n            e1.ep2.component = \"xpu[0:4]\"\n\n            # clique 4\u20137\n            e2 = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e2.ep1.component = \"xpu[4:8]\"\n            e2.ep2.component = \"xpu[4:8]\"\n\n            # cross links\n            pairs = [(0, 4), (1, 5), (2, 6), (3, 7)]\n            for a, b in pairs:\n                e = self.edges.add(DeviceEdge.ONE2ONE, self.xpu_fabric.name)\n                e.ep1.component = f\"xpu[{a}]\"\n                e.ep2.component = f\"xpu[{b}]\"\n            return\n\n        if self.profile == \"dgx2\":\n            e1 = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e1.ep1.component = \"xpu[0:8]\"\n            e1.ep2.component = \"nvlsw[0:6]\"\n\n            e2 = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e2.ep1.component = \"xpu[8:16]\"\n            e2.ep2.component = \"nvlsw[6:12]\"\n\n            e3 = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e3.ep1.component = \"nvlsw[0:6]\"\n            e3.ep2.component = \"nvlsw[6:12]\"\n            return\n\n        if self.profile == \"dgx_h100\":\n            e = self.edges.add(DeviceEdge.MANY2MANY, self.pcie.name)\n            e.ep1.component = f\"{self.nvsw.name}[0:4]\"\n            e.ep2.component = f\"{self.xpu.name}[0:8]\"\n\n        if self.profile == \"dgx_a100\":\n            # xpu to nvlink switch\n            e = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e.ep1.component = f\"{self.xpu.name}[0:8]\"\n            e.ep2.component = f\"{self.nvsw.name}[0:6]\"\n            return\n\n        if self.profile == \"dgx_gb200\":\n            # xpu to nvlink switch\n            e = self.edges.add(DeviceEdge.MANY2MANY, self.xpu_fabric.name)\n            e.ep1.component = f\"{self.xpu.name}[0:4]\"\n            e.ep2.component = f\"{self.nvsw.name}[0:18]\"\n            return\n\n    # NIC handling\n    def _add_nics(self, nic_device: Optional[NicSpec]):\n        if not self.pciesl:\n            return\n\n        if nic_device is None:\n            self._add_generic_nics()\n        elif isinstance(nic_device, str):\n            self._add_symbolic_nics(nic_device)\n        elif isinstance(nic_device, Device):\n            self._add_device_nics(nic_device)\n        else:\n            raise TypeError(\"nic_device must be None, str, or Device\")\n\n    def _add_generic_nics(self):\n        nic_name = \"nic\"\n        if self.profile == \"dgx1\" or self.profile == \"dgx2\":\n            nic_name = \"cx5\"\n        elif self.profile == \"dgx_a100\":\n            nic_name = \"cx6\"    \n        elif self.profile == \"dgx_h100\" or self.profile == \"dgx_gb200\":\n            nic_name = \"cx7\"\n\n        nic = self.components.add(\n            name=nic_name,\n            description=\"NVIDIA ConnectX / BlueField\",\n            count=self.pciesl.count,\n        )\n        nic.choice = Component.NIC\n\n        self._wire_slots_to_nics(nic_name)\n\n    def _add_symbolic_nics(self, desc: str):\n        nic = self.components.add(\n            name=desc,\n            description=desc,\n            count=self.pciesl.count,\n        )\n        nic.choice = Component.NIC\n\n        self._wire_slots_to_nics(desc)\n\n    def _add_device_nics(self, dev: Device):\n        nic = self.components.add(\n            name=dev.name,\n            description=dev.description,\n            count=self.pciesl.count,\n        )\n        nic.choice = Component.DEVICE\n\n        edge = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n        edge.ep1.device = f\"{nic.name}[0:{self.pciesl.count}]\"\n        edge.ep1.component = \"pcie_endpoint[0]\"\n        edge.ep2.component = f\"{self.pciesl.name}[0:{self.pciesl.count}]\"\n\n    def _wire_slots_to_nics(self, nic_name: str):\n        for idx in range(self.pciesl.count):\n            edge = self.edges.add(DeviceEdge.ONE2ONE, self.pcie.name)\n            edge.ep1.component = f\"{self.pciesl.name}[{idx}]\"\n            edge.ep2.component = f\"{nic_name}[{idx}]\"\n\nif __name__ == \"__main__\":\n    print(NvidiaDGX(\"dgx1\").serialize(encoding=Device.YAML))\n</code></pre> DGX device definition as yaml <pre><code>components:\n- choice: cpu\n  count: 2\n  description: AMD EPYC 9654 (Genoa)\n  name: cpu\n- choice: xpu\n  count: 8\n  description: NVIDIA H100 / H200 SXM5\n  name: xpu\n- choice: switch\n  count: 4\n  description: NVIDIA NVSwitch\n  name: nvsw\n- choice: switch\n  count: 3\n  description: Broadcom PCIe Gen5 Switch\n  name: pciesw\n- choice: custom\n  count: 8\n  custom:\n    type: pcie_slot\n  description: PCIe Gen5 x16 slots (ConnectX / BlueField)\n  name: pciesl\n- choice: nic\n  count: 8\n  description: NVIDIA ConnectX / BlueField\n  name: cx7\ndescription: NVIDIA DGX System\nedges:\n- ep1:\n    component: cpu\n  ep2:\n    component: cpu\n  link: cpu_fabric\n  scheme: many2many\n- ep1:\n    component: cpu[0]\n  ep2:\n    component: pciesl[0:3]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: cpu[1]\n  ep2:\n    component: pciesl[4:7]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: cpu[0]\n  ep2:\n    component: pciesw[0]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: cpu[1]\n  ep2:\n    component: pciesw[1]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[0]\n  ep2:\n    component: xpu[0]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[1]\n  ep2:\n    component: xpu[1]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[2]\n  ep2:\n    component: xpu[2]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[3]\n  ep2:\n    component: xpu[3]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[4]\n  ep2:\n    component: xpu[4]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[5]\n  ep2:\n    component: xpu[5]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[6]\n  ep2:\n    component: xpu[6]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[7]\n  ep2:\n    component: xpu[7]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: nvsw[0:4]\n  ep2:\n    component: pciesw[2]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: cpu[0]\n  ep2:\n    component: pciesw[2]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: nvsw[0:4]\n  ep2:\n    component: xpu[0:8]\n  link: pcie\n  scheme: many2many\n- ep1:\n    component: pciesl[0]\n  ep2:\n    component: cx7[0]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[1]\n  ep2:\n    component: cx7[1]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[2]\n  ep2:\n    component: cx7[2]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[3]\n  ep2:\n    component: cx7[3]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[4]\n  ep2:\n    component: cx7[4]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[5]\n  ep2:\n    component: cx7[5]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[6]\n  ep2:\n    component: cx7[6]\n  link: pcie\n  scheme: one2one\n- ep1:\n    component: pciesl[7]\n  ep2:\n    component: cx7[7]\n  link: pcie\n  scheme: one2one\nlinks:\n- description: infinity_fabric\n  name: cpu_fabric\n- description: nvlink_4\n  name: xpu_fabric\n- description: PCI Express PCIE_GEN5 x16\n  name: pcie\nname: dgx_h100\n</code></pre>"},{"location":"examples/#gh200-mgx","title":"GH200-MGX","text":""},{"location":"examples/#description_1","title":"Description","text":""},{"location":"examples/#xpu-component","title":"XPU Component","text":""},{"location":"examples/#device","title":"Device","text":""},{"location":"examples/#standardized-definition_1","title":"Standardized Definition","text":"GH200-MGX device definition using OpenApiArt generated classes <pre><code>TBD...\n</code></pre>"},{"location":"examples/#scaleupscaleout-infrastructure","title":"ScaleUp/ScaleOut Infrastructure","text":""},{"location":"examples/#description_2","title":"Description","text":"<p> https://mips.com/blog/reimagining-ai-infrastructure-the-power-of-converged-back-end-networks/</p> <ul> <li>1024 hosts<ul> <li>1 xpu/host</li> <li>10 nics/host</li> </ul> </li> <li>512 scaleup switches<ul> <li>16 ports/switch</li> </ul> </li> <li>2 scaleout switches<ul> <li>1024 ports/switch</li> </ul> </li> <li>64 Racks<ul> <li>16 hosts/rack</li> <li>8 scale up switches/rack</li> </ul> </li> </ul> <p></p>"},{"location":"examples/#standardized-definition_2","title":"Standardized Definition","text":"ScaleUp/ScaleOut infrastructure definition using OpenApiArt generated classes <pre><code>TBD...\n</code></pre>"},{"location":"model/","title":"Model","text":""},{"location":"model/#formal-model-infrastructure-as-a-graph","title":"Formal Model - Infrastructure As A Graph","text":"<p>The formal model specification can be found on GitHub under Infrastructure organization. The model has been defined as a protobuf message because Protocol Buffers provide a highly efficient, compact, and language-neutral way to serialize structured data. This binary serialization format results in significantly smaller message sizes compared to text-based formats like JSON or YAML, which reduces network bandwidth usage and storage requirements</p>"},{"location":"model/#building-blocks","title":"Building Blocks","text":"<p>The infra.proto provides multiple building blocks to define the infrastructure. These blocks include:</p> <ul> <li>Inventory</li> <li>Device</li> <li>Components</li> <li>Links</li> <li>Device Instances</li> <li>Connections</li> </ul>"},{"location":"model/#devices","title":"Devices","text":"<pre><code>message Device {\n  optional string name = 1;\n  map&lt;string, Component&gt; components = 3;\n  map&lt;string, Link&gt; links = 4;\n  repeated string connections = 5;\n}\n</code></pre> <p>The Device message defines a device which is a part of the infrastructure. This contains a collection of components, links between the components and the connections. The main fields are: * name: An optional field allowing users to define the name of the device * components: A dictionary which stores the component message with the key as the component name. This message is defined in the later section * links: Another dictionary which stores the link message with the link name.  * connections: A list of connections that describe how components are connected to each other in a single device. Each element of this list is a string that describe the component connection and is described as:     <pre><code>source_component_name \".\" source_component_index \".\" link_name \".\" destination_component_name \".\" destination_component_index \n</code></pre>     example:     <pre><code>nic.0.pcie.cpu.0\nxpu.0.pcie.nvswitch.0\nasic.0.mii.nic.0\n</code></pre>     The source_component_name and destination_component_name is the name field present in the component message. This name also corresponds to the key of the components dictionary field which is a part of the device message. Each component message holds a count field which defines the number of components present in the device. These fields are defined later in the components section.</p>"},{"location":"model/#components","title":"Components","text":"<pre><code>message Component {\n  optional string name = 1;\n  optional uint32 count = 2;\n  oneof type {\n    CustomComponent custom = 10;\n    Cpu cpu = 11;\n    Xpu xpu = 12;\n    Nic nic = 13;\n    Switch switch = 14;\n  }\n}\n</code></pre> <p>The component message defines three major fields: * name: An optional field which gives the name of the component. The name is also provided as a key in the components dictionary field type of device message. * count: The count defines the total components present. Lets assume we have a nic component with a count of 8. This would create 8 instances of the nic component whose properies would remain the same with a zero based indexing. This is analogous to the concept of classes and objects where the component message acts as the blueprint and count indicates the number of objects created.  * type: The component datamodel allows to describe component of a certain type. The type can be:     * CPU     * XPU     * NIC     * Switch     * Custom</p> <pre><code>Each of these types are defined as another message. The section below describes the message format of component type.\n</code></pre>"},{"location":"model/#cpu-component","title":"CPU Component","text":"<p><pre><code>message Cpu {\n  MemoryType memory = 1;\n}\n</code></pre> This message defines the CPU type component. This allows the user to assign a certain memory type to the CPU. The MemoryType is covered in the later section.</p>"},{"location":"model/#xpu-component","title":"XPU Component","text":"<pre><code>message Npu {\n  MemoryType memory = 1;\n}\n</code></pre> <p>This message defines the XPU type component. This allows the user to assign a certain memory type to the XPU. The MemoryType is covered in the later section.</p>"},{"location":"model/#custom-component","title":"Custom Component","text":"<pre><code>message CustomComponent {\n  MemoryType memory = 1;\n}\n</code></pre> <p>This message defines the CustomComponent type component. This allows the user to assign a certain memory type to the Custom Component. The MemoryType is covered in the later section.</p>"},{"location":"model/#memory-type","title":"Memory Type","text":"<p><pre><code>enum MemoryType {\n  MEM_UNSPECIFIED = 0;\n\n  // random access memory\n  MEM_RAM = 1;\n\n  // high bandwidth memory interface for 3D stacked sync dynamic random-access memory\n  MEM_HBM = 2;\n\n  // memory that uses compute express link interconnect to the cpu\n  MEM_CXL = 3;\n}\n</code></pre> The user can set either of the memory type to the CPU, XPU or CustomComponent Type. The memory could be either: * Unspecified * Random Access Memory * High Bandwidth Memory Interface * Compute Express Link</p> <p>The enum can be extended to add more memory types which can be used by custom component</p>"},{"location":"model/#nic-component","title":"NIC Component","text":"<pre><code>message Nic {\n  oneof type {\n    Ethernet ethernet = 10;\n    Infiniband infinband = 11;\n  }\n}\n\nmessage Infiniband {\n}\n\nmessage Ethernet {\n}\n</code></pre> <p>This describes the NIC Component. Each nic component can be of the following type: * Ethernet * Infiniband These types are defined as a message. </p>"},{"location":"model/#switch-component","title":"Switch Component","text":"<pre><code>message Switch {\n  oneof type {\n    Pcie pcie = 1;\n    NvLink nvswitch = 2;\n    Custom custom = 3;\n  }\n}\n\nmessage Pcie {\n}\n\nmessage NvLink {\n}\n\nmessage Custom {\n}\n</code></pre> <p>This section defines the Switch Component type. The switch component can be either of the following: * pcie * nvlink * custom</p> <p>These types are defined as a message. </p>"},{"location":"model/#link","title":"Link","text":"<pre><code>message Link {\n  optional string name = 1;\n  optional string description = 2;\n  Bandwidth bandwidth = 10;\n}\n\nmessage Bandwidth {\n  oneof type {\n    uint32 gbps = 1;\n    uint32 gBs = 2;\n    uint32 gts = 3;\n  }\n}\n</code></pre> <p>The Link message allows to define a \"link\" between the device as well as components. This model has three fields: * name * description * bandwidth</p> <p>The Bandwidth is defined as a message and allows to define the link bandwidth as: * gbps: gigabits per second * gBs: gigabytes per second * gts: giga transfers per second</p> <p>These take an unsigned integer value. The Links use the Bandwidth message model to define the link speed. </p>"},{"location":"model/#inventory","title":"Inventory","text":"<pre><code>message Inventory {\n  map&lt;string, Device&gt; devices = 1;\n  map&lt;string, Link&gt; links = 2;\n}\n</code></pre> <p>Inventory is a collection of all unique types of Devices and Links in the infrastructure. This has two major fields: * devices: </p> <pre><code>A collection of all unique types of devices in the infrastructure. The uniqueness is determined by the Device.name field.\n</code></pre> <ul> <li> <p>links:</p> <p>A collection of all unique types of links in the infrastructure. These links can be reused multiple times when creating connections between devices. The key is the Link.name which is used to guard against duplicates.</p> </li> </ul>"},{"location":"model/#device-instances","title":"Device Instances","text":"<pre><code>message DeviceInstances {\n  optional string name = 1;\n  optional string device = 2;\n  optional uint32 count = 3;\n}\n</code></pre> <p>The Device Instances message is used to instantiate the Device in the infrastructure. This message contains three fields: * name: the name of the device instance. This is used to categorize the device. For example: a switch defined in the inventory - devices message can be used as a Rack Switch, POD Switch or a Spine Switch. The name allows to create/provide a unique name to a set of devices.  * device: The name of the actual device that exist in the inventory - devices  field. This links the device which we want to use. * count: The number of instances of device in the infrastructure under this name. This should always be &gt;= 1. This also indiates the number of instances we need for a specific device under a certain name. This is again analogous to component modelling but done on a device level. The indexing starts at 0 and provides a unique identifier to create a device instance.</p>"},{"location":"model/#infrastructure","title":"Infrastructure","text":"<pre><code>message Infrastructure {\n  Inventory inventory = 1;\n  map&lt;string, DeviceInstances&gt; device_instances = 2;\n  repeated string connections = 3;\n}\n</code></pre> <p>The Infrastructure message establishes an inventory of devices and links, instances of the inventory, connectivity between those instances and any custom user information about devices, components, links and instances.</p> <p>This holds the inventory which contains the devices, links; the device_instances map that hold all the devices instantiated and a list of connections. The connection format is defined as a string of the following elements separated by a \".\"</p> <pre><code>source_device_instance_name\nsource_device_index \nsource_device_component_name\nsource_device_component_index\nlink_name\ndestination_device_instance_name\ndestination_device_index\ndestination_device_component_name\ndestination_device_component_index\n</code></pre> <p>This utilizes the device instances naming convention with the count which internally links to the  device message that defines the component, its name and the count and connects two different device instances with the link name that is defined in the inventory. </p>"},{"location":"model/#building-infrastructure","title":"Building Infrastructure","text":"<p>The model allows us to define devices, its internal components, as nodes and links as edges and creates connection as a link between the nodes thereby allowing to create a graph based representation. A step by step guide to create infrastructure is defined in Building A Cluster.</p>"},{"location":"schema/","title":"Schema","text":""},{"location":"schema/#introducing-the-infragraph-infrastructure-graph-schema","title":"Introducing the InfraGraph (INFRAstructure GRAPH) schema","text":"<p>A graph is a natural fit to describe a system of systems in a clear, intuitive, and mathematically precise manner.</p> <p></p> <ul> <li>Node or vertex represents an entity like a component, device, user, router, etc</li> <li>Edge represents a relationship between nodes</li> <li>Properties store additional information about nodes or edges</li> </ul>"},{"location":"schema/#principles","title":"Principles","text":"<p>InfraGraph is a <code>collection of APIs and Models</code> used to describe AI/HPC infrastructure based on the following core principles:</p> <ul> <li>infrastructure can be described using graph concepts such as vertexes, edges and properties<ul> <li>vertexes can be <code>component</code> or device <code>instances</code></li> <li>an edge contained by a device subgraph is 2 component instances separated by a link<ul> <li>e.g., <code>xpu.0</code>.<code>pcie</code>.<code>nic.0</code></li> </ul> </li> <li>an edge contained by the infrastructure is 2 device connections separated by a link where a device connection is the device <code>instance</code> name and <code>index</code> and external <code>component</code> name and <code>index</code><ul> <li>e.g., <code>server.0</code>.<code>nic.0</code>.<code>eth</code>.<code>leafsw.0</code>.<code>port.0</code></li> </ul> </li> <li>a path is a collection of infrastructure and device connections between a single source and destination</li> <li>properties are fields with in the device and component objects</li> </ul> </li> <li>devices are composable using connections</li> <li>connections dictate the depth of the graph</li> <li>infrastructure and device connections dictate the shape of the graph</li> <li>infrastructure needs to be scalable without duplicating content</li> </ul>"},{"location":"schema/#openapiart","title":"OpenapiArt","text":"<p>This repository makes use of OpenAPIArt to do the following:</p> <ul> <li>create declarative intent based Models and APIs</li> <li>auto-generate the following artifacts:<ul> <li>openapi schema</li> <li>protobuf schema</li> <li>Redocly documentation of APIs and Models</li> <li><code>Python/Go SDKs</code> that allow for creating <code>fluent</code> client/server code over <code>REST/Protobuf</code> transports</li> </ul> </li> </ul>"},{"location":"unused/","title":"Unused","text":""},{"location":"unused/#features","title":"Features","text":"<p>Cluster Infrastructure as a graph is an actively developed specification, with contributions from real use cases. The model defines the following components to define a infrastructure:</p> <ul> <li>Device definitions with ability to model its internals as a graph</li> <li>Device Components allowing users to define the device internal components like:<ul> <li>nic</li> <li>ports</li> <li>npus</li> </ul> </li> <li>Links definition for:<ul> <li>components interconnect</li> <li>device interconnect</li> <li>Defining the bandwidth of the links</li> </ul> </li> <li>Connections between:<ul> <li>internal components of a device</li> <li>one device to another</li> </ul> </li> </ul> <p>Explore an in-depth explanation of the topology model, covering its structure, essential components, and how it supports efficient design and analysis. This resource provides valuable insights into the principles behind topology and how to apply them effectively.</p>"},{"location":"unused/#annotation","title":"Annotation","text":"<p>This section provides a comprehensive guide on how a user can annotate various parts of infrastructre and add more details like DeviceType, Rank Identifier and so on. It covers the model description with examples for binding physical attributes with the logical infrastructure definition.</p>"},{"location":"unused/#getting-started-with-topology-creation","title":"Getting Started With Topology Creation","text":"<p>This walkthrough guide demonstrates how anyone can create a topology from scratch, highlighting key steps and best practices to build a solid foundation. It offers a clear, step-by-step approach that makes topology creation accessible to beginners and experts alike.</p>"},{"location":"unused/#community","title":"Community","text":"<p>Use our community resources to get help with Infrastructure As A Graph:</p> <ul> <li>Infrastructure As A Graph on Github</li> </ul>"},{"location":"unused/#infrastructure-connections","title":"Infrastructure Connections","text":"<ul> <li>Create Connection between Components using Links</li> <li> <p>Defining External Links: Here we define the external link type that connects two devices</p> </li> <li> <p>Building the infrastructure as graph</p> </li> <li>Instantiating devices: Use the device definition in the inventory as a template to create multiple devices for the infrastructure.</li> <li>Defining connections: Use the external link definition to create connections between device instances.</li> </ul> <p>Follow these steps to design a Scale Up and Out Infrastructure.</p>"},{"location":"unused/#creating-device-inventory","title":"Creating Device Inventory","text":"<p>Device inventory outlines the necessary devices for infrastructure, including components and links. It acts as a blueprint to create and connect instances, aiming to define once and reuse multiple times for optimal space complexity. For example, in a network with 100 switches (50 each of 2 types) connected by 100G ethernet links, the inventory will only specify the 2 switch types and the 100G ethernet link type.</p> <p>Note that the entire device does not need to be described in full detail. The level of device detail should be dictated by the needs of the application.</p> <p>To define a Device:</p> <ul> <li>use the <code>Component</code> message to define individual components (vertexes) that are present in a device</li> <li>use the <code>Component - count</code> field to scale up the number of components in the device</li> <li>use the <code>Link</code> message to define different link types within the device</li> <li>use the <code>Device</code> message to contain <code>Component</code> and <code>Link</code> messages</li> <li>use the <code>Device - connections</code> field to connect components (vertexes) to each other with an associated link to form an edge</li> <li>the format of a <code>connections</code> string is described in the infra.proto file</li> </ul> <p>Now we will be designing a 4 port generic switch as a part of device inventory.</p>"},{"location":"unused/#defining-2-port-scale-up-switch","title":"Defining 2 port scale up switch","text":"<p>Lets define a simple 2 port scale up switch. </p> <p>This switch is made of two front panel ports.</p> <p>User can define this switch with one major \"port\" components inside it. These components can be viewed as nodes in a graph.</p> YAML Definition <pre><code>inventory:\n  devices:\n    SCALE_UP_SWITCH:\n      name: SCALE_UP_SWITCH\n      components:\n        port:\n          count: 2\n          name: port\n          nic:\n            ethernet: {}\n      connections: []\n      links: {}\n</code></pre> <p></p> JSON Definition <pre><code>{\n  \"inventory\": {\n    \"devices\": {\n      \"SCALE_UP_SWITCH\": {\n        \"name\": \"SCALE_UP_SWITCH\",\n        \"components\": {\n            \"port\": {\n                \"name\": \"port\",\n                \"count\": 2,\n                \"nic\": {\n                    \"ethernet\": { }\n                }\n            }\n        },\n        \"links\": {\n        },\n        \"connections\": [\n        ]\n    }\n  }\n}\n</code></pre> <p></p> <p>We have specified a scale up switch with 2 port components. Now we can define a scale out switch with 4 port components.</p>"},{"location":"unused/#defining-4-port-scale-out-switch","title":"Defining 4 port scale out switch","text":"<p>Lets define a simple 2 port scale up switch. </p> <p>This switch is made of four front panel ports.</p> <p>User can define this switch with one major \"port\" components inside it. These components are analogous to a node in a graph.</p> YAML Definition <pre><code>inventory:\n  devices:\n    SCALE_OUT_SWITCH:\n      name: SCALE_OUT_SWITCH\n      components:\n        port:\n          count: 4\n          name: port\n          nic:\n            ethernet: {}\n      connections: []\n      links: {}\n</code></pre> <p></p> JSON Definition <pre><code>{\n    \"inventory\": {\n        \"devices\": {\n            \"SCALE_OUT_SWITCH\": {\n                \"name\": \"SCALE_OUT_SWITCH\",\n                \"components\": {\n                    \"port\": {\n                        \"name\": \"port\",\n                        \"count\": 4,\n                        \"nic\": {\n                            \"ethernet\": { }\n                        }\n                    }\n                },\n                \"links\": {\n                },\n                \"connections\": [\n                ]\n            },\n        }\n    }\n}\n</code></pre> <p></p>"},{"location":"unused/#design-host-with-4-nics-and-single-xpu","title":"Design host with 4 nics and single xpu","text":"<p>Let's design a host with 4 nics and a single xpu</p> <p></p> <p>Our Host has two interconnected components: - 4 nics - 1 xpu</p> <p>These components are connected to each other via a pcie connection. Therefore to connect two different components, we can use the following notation:</p> <p><code>&lt;source&gt;.&lt;link&gt;.&lt;destination&gt;</code></p> <p>The <code>&lt;source&gt;</code> contains the source component and its index. The <code>&lt;destination&gt;</code> specifies the destination component and its index. The link joins the source and the destination. Therefore, the connection would look something like this:</p> <pre><code>&lt;source&gt;.&lt;src_index&gt;.&lt;link&gt;.&lt;destination&gt;.&lt;dst_index&gt;\n</code></pre> <p>These components can be defined as node in a graph which are connected through an edge (pcie) link in this case. We need to define the links as well as connections with which the whole device definition is defined below:</p> YAML Definition <pre><code>inventory:\n  devices:\n    HOST:\n      name: HOST\n      components:\n        nic:\n          name: nic\n          count: 4\n          nic:\n            ethernet: {}\n        xpu:\n          name: xpu\n          count: 1\n          xpu: {}\n      links:\n        pcie:\n          name: pcie\n      connections:\n      - xpu.0.pcie.nic.0\n      - xpu.0.pcie.nic.1\n      - xpu.0.pcie.nic.2\n      - xpu.0.pcie.nic.3\n</code></pre> <p></p> JSON Definition <pre><code>{\n    \"inventory\": {\n        \"devices\": {\n            \"HOST\": {\n                \"name\": \"HOST\",\n                \"components\": {\n                    \"xpu\": {\n                        \"name\": \"xpu\",\n                        \"count\": 1,\n                        \"xpu\": { }\n                    },\n                    \"nic\": {\n                        \"name\": \"nic\",\n                        \"count\": 4,\n                        \"nic\": {\n                            \"ethernet\": { }\n                        }\n                    }\n                },\n                \"links\": {\n                    \"pcie\": {\n                        \"name\": \"pcie\"\n                    }\n                },\n                \"connections\": [\n                    \"xpu.0.pcie.nic.0\",\n                    \"xpu.0.pcie.nic.1\",\n                    \"xpu.0.pcie.nic.2\",\n                    \"xpu.0.pcie.nic.3\"\n                ]\n            }\n        }\n    }\n}\n</code></pre> <p></p>"},{"location":"unused/#defining-links","title":"Defining Links","text":"<p>The objective is to define an infrastructure build using the switch and hosts defined earlier. The goal is to build an infrastructure where one switch is directly connected to four hosts via 100G Ethernet.</p> <p></p> <p>We have defined a switch and a host in the inventory, but not the 100G links. Let's define a 100G ethernet link as follows:</p> YAML Definition <pre><code>inventory:\n  links:\n  eth:\n    name: eth\n    description: Ethernet link\n    bandwidth:\n      gbps: 100\n</code></pre> <p></p> JSON Definition <pre><code>{\n  \"inventory\": {\n    \"links\": {\n      \"eth\": {\n        \"name\": \"eth\",\n        \"description\": \"Ethernet link\",\n        \"bandwidth\": {\n          \"gbps\": 100\n        }\n      }\n    }\n  }\n}\n</code></pre> <p></p> <p>In this example, we have defined a link <code>name: eth</code> with a bandwidth of 100 gbps. Subsequently, four such links will be utilized to connect four devices to four switch ports, as illustrated in the above image.</p>"},{"location":"unused/#creating-device-instances","title":"Creating Device Instances","text":"<p>We can scale the infrastructure by using the <code>device instance</code> message. To create a fully connected topology, we instantiate the defined devices by giving a new <code>instance_name</code> to the device followed by a count. Therefore to create instances for host, scale up and scale out switch, we define the instance as following:</p> YAML Definition <pre><code>deviceInstances:\n  host:\n    count: 4\n    device: HOST\n    name: host\n  sosw:\n    count: 2\n    device: SCALE_OUT_SWITCH\n    name: sosw\n  susw:\n    count: 4\n    device: SCALE_UP_SWITCH\n    name: susw\n</code></pre> <p></p> JSON Definition <pre><code>{\n    \"deviceInstances\": {\n        \"host\": {\n            \"name\": \"host\",\n            \"device\": \"HOST\",\n            \"count\": 4\n        },\n        \"susw\": {\n            \"name\": \"susw\",\n            \"device\": \"SCALE_UP_SWITCH\",\n            \"count\": 4\n        },\n        \"sosw\": {\n            \"name\": \"sosw\",\n            \"device\": \"SCALE_OUT_SWITCH\",\n            \"count\": 2\n        }\n    }\n}\n</code></pre> <p></p> <p>The devices are defined under the <code>inventory - devices</code> section, serving as a blueprint or template. These devices need to be instantiated to create the entire infrastructure, similar to creating objects of a class. With the specified count, multiple copies of the devices are created starting from index 0.</p> <p>Next, these device instances need to be connected over 100G ethernet links as illustrated in the picture above.</p>"},{"location":"unused/#connecting-device-instances","title":"Connecting Device Instances","text":"<p>Connections between the devices are made by the components of the device and links defined. Therefore, to connect two devices together, we need to define the connection in the following format:</p> <pre><code>&lt;src_device&gt;.&lt;dev_index&gt;&lt;src_component&gt;&lt;comp_index&gt;.&lt;link&gt;.&lt;dst_device&gt;.&lt;dev_index&gt;&lt;dst_component&gt;&lt;comp_index&gt;\n</code></pre> <p>The <code>&lt;src_device&gt;.&lt;dev_index&gt;&lt;src_component&gt;&lt;comp_index&gt;</code> specifies the source device, its index, component, and the component's index. The same format applies to the destination. The link defines the connection between source and destination.</p> <p>A \".\" separator separates infrastructure elements. To connect a <code>host</code> with the <code>scale_up_switch</code>, we define the connection as:</p> YAML Definition <pre><code>connections:\n  - host.0.nic.0.eth.susw.0.port.0\n</code></pre> <p></p> JSON Definition <pre><code>{\n  \"connections\": [\"host.0.nic.0.eth.susw.0.port.0\"]\n}\n</code></pre> <p></p> <p>The host at index 0, via its nic component 0, is connected to port 0 of scale up switch or <code>susw</code> 0. The link between this source and destination has a bandwidth of eth. This describes the first link shown in the above picture.</p> <p>Creating the links:</p> YAML Definition <pre><code>connections:\n    - host.0.nic.0.eth.susw.0.port.0\n    - host.0.nic.1.eth.susw.1.port.0\n    - host.0.nic.2.eth.sosw.0.port.0\n    - host.0.nic.3.eth.sosw.1.port.0\n    - host.1.nic.0.eth.susw.0.port.1\n    - host.1.nic.1.eth.susw.1.port.1\n    - host.1.nic.2.eth.sosw.0.port.1\n    - host.1.nic.3.eth.sosw.1.port.1\n    - host.2.nic.0.eth.susw.2.port.0\n    - host.2.nic.1.eth.susw.3.port.0\n    - host.2.nic.2.eth.sosw.0.port.2\n    - host.2.nic.3.eth.sosw.1.port.2\n    - host.3.nic.0.eth.susw.2.port.1\n    - host.3.nic.1.eth.susw.3.port.1\n    - host.3.nic.2.eth.sosw.0.port.3\n    - host.3.nic.3.eth.sosw.1.port.3\n</code></pre> <p></p> JSON Definition <pre><code>{\n  \"connections\": [\n        \"host.0.nic.0.eth.susw.0.port.0\",\n        \"host.0.nic.1.eth.susw.1.port.0\",\n        \"host.0.nic.2.eth.sosw.0.port.0\",\n        \"host.0.nic.3.eth.sosw.1.port.0\",\n        \"host.1.nic.0.eth.susw.0.port.1\",\n        \"host.1.nic.1.eth.susw.1.port.1\",\n        \"host.1.nic.2.eth.sosw.0.port.1\",\n        \"host.1.nic.3.eth.sosw.1.port.1\",\n        \"host.2.nic.0.eth.susw.2.port.0\",\n        \"host.2.nic.1.eth.susw.3.port.0\",\n        \"host.2.nic.2.eth.sosw.0.port.2\",\n        \"host.2.nic.3.eth.sosw.1.port.2\",\n        \"host.3.nic.0.eth.susw.2.port.1\",\n        \"host.3.nic.1.eth.susw.3.port.1\",\n        \"host.3.nic.2.eth.sosw.0.port.3\",\n        \"host.3.nic.3.eth.sosw.1.port.3\"\n    ]\n}\n</code></pre> <p></p>"},{"location":"unused/#the-complete-example","title":"The Complete Example","text":"<p>After combining all the definitions, we can arrive at the final design:</p> YAML Definition <pre><code>inventory:\n  devices:\n    HOST:\n      name: HOST\n      components:\n        nic:\n          name: nic\n          count: 4\n          nic:\n            ethernet: {}\n        xpu:\n          name: xpu\n          count: 1\n          xpu: {}\n      links:\n        pcie:\n          name: pcie\n      connections:\n      - xpu.0.pcie.nic.0\n      - xpu.0.pcie.nic.1\n      - xpu.0.pcie.nic.2\n      - xpu.0.pcie.nic.3\n    SCALE_OUT_SWITCH:\n      name: SCALE_OUT_SWITCH\n      components:\n        port:\n          name: port\n          count: 4\n          nic:\n            ethernet: {}\n      links: {}\n      connections: []\n    SCALE_UP_SWITCH:\n      name: SCALE_UP_SWITCH\n      components:\n        port:\n          name: port\n          count: 2\n          nic:\n            ethernet: {}\n      links: {}\n      connections: []\n  links:\n    eth:\n      name: eth\n      bandwidth:\n        gbps: 100\n      description: Ethernet link\ndeviceInstances:\n  host:\n    name: host\n    device: HOST\n    count: 4\n  sosw:\n    name: sosw\n    device: SCALE_OUT_SWITCH\n    count: 2\n  susw:\n    name: susw\n    device: SCALE_UP_SWITCH\n    count: 4\nconnections:\n- host.0.nic.0.eth.susw.0.port.0\n- host.0.nic.1.eth.susw.1.port.0\n- host.0.nic.2.eth.sosw.0.port.0\n- host.0.nic.3.eth.sosw.1.port.0\n- host.1.nic.0.eth.susw.0.port.1\n- host.1.nic.1.eth.susw.1.port.1\n- host.1.nic.2.eth.sosw.0.port.1\n- host.1.nic.3.eth.sosw.1.port.1\n- host.2.nic.0.eth.susw.2.port.0\n- host.2.nic.1.eth.susw.3.port.0\n- host.2.nic.2.eth.sosw.0.port.2\n- host.2.nic.3.eth.sosw.1.port.2\n- host.3.nic.0.eth.susw.2.port.1\n- host.3.nic.1.eth.susw.3.port.1\n- host.3.nic.2.eth.sosw.0.port.3\n- host.3.nic.3.eth.sosw.1.port.3\n</code></pre> <p></p> JSON Definition <pre><code>{\n    \"inventory\": {\n        \"devices\": {\n            \"SCALE_OUT_SWITCH\": {\n                \"name\": \"SCALE_OUT_SWITCH\",\n                \"components\": {\n                    \"port\": {\n                        \"name\": \"port\",\n                        \"count\": 4,\n                        \"nic\": {\n                            \"ethernet\": { }\n                        }\n                    }\n                },\n                \"links\": {\n                },\n                \"connections\": [\n                ]\n            },\n            \"SCALE_UP_SWITCH\": {\n                \"name\": \"SCALE_UP_SWITCH\",\n                \"components\": {\n                    \"port\": {\n                        \"name\": \"port\",\n                        \"count\": 2,\n                        \"nic\": {\n                            \"ethernet\": { }\n                        }\n                    }\n                },\n                \"links\": {\n                },\n                \"connections\": [\n                ]\n            },\n            \"HOST\": {\n                \"name\": \"HOST\",\n                \"components\": {\n                    \"xpu\": {\n                        \"name\": \"xpu\",\n                        \"count\": 1,\n                        \"xpu\": { }\n                    },\n                    \"nic\": {\n                        \"name\": \"nic\",\n                        \"count\": 4,\n                        \"nic\": {\n                            \"ethernet\": { }\n                        }\n                    }\n                },\n                \"links\": {\n                    \"pcie\": {\n                        \"name\": \"pcie\"\n                    }\n                },\n                \"connections\": [\n                    \"xpu.0.pcie.nic.0\",\n                    \"xpu.1.pcie.nic.1\",\n                    \"xpu.2.pcie.nic.2\",\n                    \"xpu.3.pcie.nic.3\"\n                ]\n            }\n        },\n        \"links\": {\n            \"eth\": {\n                \"name\": \"eth\",\n                \"description\": \"Ethernet link\",\n                \"bandwidth\": {\n                    \"gbps\": 100\n                }\n            }\n        }\n    },\n    \"deviceInstances\": {\n        \"host\": {\n            \"name\": \"host\",\n            \"device\": \"HOST\",\n            \"count\": 4\n        },\n        \"susw\": {\n            \"name\": \"susw\",\n            \"device\": \"SCALE_UP_SWITCH\",\n            \"count\": 4\n        },\n        \"sosw\": {\n            \"name\": \"sosw\",\n            \"device\": \"SCALE_OUT_SWITCH\",\n            \"count\": 2\n        }\n    },\n    \"connections\": [\n        \"host.0.nic.0.eth.susw.0.port.0\",\n        \"host.0.nic.1.eth.susw.1.port.0\",\n        \"host.0.nic.2.eth.sosw.0.port.0\",\n        \"host.0.nic.3.eth.sosw.1.port.0\",\n        \"host.1.nic.0.eth.susw.0.port.1\",\n        \"host.1.nic.1.eth.susw.1.port.1\",\n        \"host.1.nic.2.eth.sosw.0.port.1\",\n        \"host.1.nic.3.eth.sosw.1.port.1\",\n        \"host.2.nic.0.eth.susw.2.port.0\",\n        \"host.2.nic.1.eth.susw.3.port.0\",\n        \"host.2.nic.2.eth.sosw.0.port.2\",\n        \"host.2.nic.3.eth.sosw.1.port.2\",\n        \"host.3.nic.0.eth.susw.2.port.1\",\n        \"host.3.nic.1.eth.susw.3.port.1\",\n        \"host.3.nic.2.eth.sosw.0.port.3\",\n        \"host.3.nic.3.eth.sosw.1.port.3\"\n    ]\n}\n</code></pre> <p></p>"},{"location":"version/","title":"Infragraph Versioning","text":"<p>Infragraph uses semantic versioning in the format MAJOR.MINOR.PATCH, where each component is a non-negative integer without leading zeroes.</p> <ul> <li>MAJOR \u2014 Increment for incompatible API changes (reset MINOR and PATCH).  </li> <li>MINOR \u2014 Increment for backward-compatible feature additions (reset PATCH).  </li> <li>PATCH \u2014 Increment for backward-compatible bug fixes.  </li> </ul> <p>Once released, a versioned package must remain immutable. All changes require a new version.</p>"},{"location":"version/#version-update-policy","title":"Version Update Policy","text":"<p>The Infragraph version is maintained in api/info.yaml. Any version change must be made exclusively in this file, and the modification must be included in a dedicated merge request (MR).</p>"},{"location":"version/#version-bumping-rules","title":"Version Bumping Rules","text":"<p>A version bump is required whenever there is a change to the API or models. The version follows the <code>MAJOR.MINOR.PATCH</code> format, updated according to the following rules:</p> <ul> <li>MAJOR: Incremented for incompatible schema or API changes, such as removing or renaming schema entities or endpoints. When the MAJOR version is incremented, both MINOR and PATCH values are reset to zero.  </li> <li>MINOR: Incremented for backward-compatible feature additions, such as introducing new schemas or APIs that do not break existing functionality. In this case, PATCH is reset to zero.  </li> <li>PATCH: Incremented for backward-compatible fixes, small corrections, or documentation adjustments that do not alter the schema interface.</li> </ul>"},{"location":"version/#change-management","title":"Change Management","text":"<p>Each schema change that requires a version bump must be submitted in a standalone MR. This separation ensures strict adherence to semantic versioning principles and maintains clear traceability of version history.</p>"},{"location":"version/#immutability-of-releases","title":"Immutability of Releases","text":"<p>Once a versioned package is released, it becomes immutable. No further modifications can be made to that version. Any subsequent change must occur through a new version bump as defined above.</p> <p>This process enforces strong semantic versioning discipline, ensuring version numbers accurately represent schema compatibility and that every schema or API update is clearly communicated through controlled, versioned releases.</p>"}]}